{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkflQpirJG_2",
        "outputId": "03f90526-1679-45d2-837b-16ed3d7805d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'OHO'...\n",
            "remote: Enumerating objects: 114, done.\u001b[K\n",
            "remote: Counting objects: 100% (114/114), done.\u001b[K\n",
            "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
            "remote: Total 114 (delta 55), reused 73 (delta 29), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (114/114), 511.40 KiB | 1.99 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jiwoongim/OHO.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu6FJY_qJJJm",
        "outputId": "058a640d-07f4-4dda-9d31-b6c5ef5262ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/OHO\n",
            "Obtaining file:///content/OHO\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: metaopt\n",
            "  Running setup.py develop for metaopt\n",
            "Successfully installed metaopt-1.0\n",
            "/content\n",
            "/content/OHO\n"
          ]
        }
      ],
      "source": [
        "%cd OHO\n",
        "!pip install -e .\n",
        "%cd ..\n",
        "%mkdir save_dir\n",
        "%cd OHO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N99ILmDDJLoK",
        "outputId": "57ab499e-7bc9-4848-efc6-548bf7bd46f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/OHO/metaopt/cifar\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from metaopt.cifar.main import load_cifar10\n",
        "except:\n",
        "    %cd metaopt/cifar\n",
        "    from metaopt.cifar.main import load_cifar10\n",
        "from metaopt.util import *\n",
        "from metaopt.util_ml import *\n",
        "from metaopt.cifar.resnet18 import *\n",
        "\n",
        "import os, sys, math, argparse, time, pickle\n",
        "import torch, torch.nn as nn, torch.optim as optim, numpy as np\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from copy import copy\n",
        "from itertools import tee\n",
        "from operator import mul\n",
        "import torch.backends.cudnn as cudnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMC1ShWVJQ2v",
        "outputId": "401a2bbf-f551-44d0-b30a-607ff28b561e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 42.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "parser = argparse.ArgumentParser(description='Example parser')\n",
        "parser.add_argument('--valid_size', type=int, default=10000)\n",
        "parser.add_argument('--batch_size', type=int, default=100)\n",
        "parser.add_argument('--is_cuda', type=int)\n",
        "parser.add_argument('--batch_size_vl', type=int)\n",
        "args = parser.parse_args(['--valid_size', '10000', '--batch_size', '100', '--is_cuda', '1', '--batch_size_vl', '1000'])\n",
        "dataset = load_cifar10(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjYwKsyyJ1rV"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, \\\n",
        "                num_classes=10, lr_init=0.00001, is_cuda=1):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "        self.param_shapes = [tuple(p.shape) for p in self.parameters()]\n",
        "        self.param_sizes = [p.numel() for p in self.parameters()]\n",
        "        self.param_cumsum = np.cumsum([0] + self.param_sizes)\n",
        "        self.n_params = sum(self.param_sizes)\n",
        "\n",
        "        self.reset_jacob(is_cuda)\n",
        "        self.eta  = lr_init\n",
        "        self.name = 'REZ'\n",
        "        self.grad_norm = 0\n",
        "        self.grad_norm_vl = 0\n",
        "        self.grad_angle = 0\n",
        "        self.param_norm = 0\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "    def reset_jacob(self, is_cuda):\n",
        "        self.dFdlr = torch.zeros(self.n_params)\n",
        "        self.dFdlr_norm = 0\n",
        "        if is_cuda:\n",
        "            self.dFdlr = self.dFdlr.cuda()\n",
        "\n",
        "    def update_dFdlr(self, Hv, param, grad, is_cuda=0, opt_type='sgd', noise=None, N=50000):\n",
        "\n",
        "        self.Hlr = self.eta*Hv\n",
        "        self.Hlr_norm = norm(self.Hlr)\n",
        "        self.dFdlr_norm = norm(self.dFdlr)\n",
        "        self.dFdlr.data = self.dFdlr.data * (1-2*self.lambda_l2*self.eta) \\\n",
        "                                - self.Hlr - grad - 2*self.lambda_l2*param\n",
        "        if opt_type == 'sgld':\n",
        "            if noise is None: noise = torch.randn(size=param.shape)\n",
        "            self.dFdlr.data = self.dFdlr.data + 0.5 * torch.sqrt(2*noise / self.eta / N)\n",
        "\n",
        "\n",
        "    def update_eta(self, mlr, val_grad):\n",
        "\n",
        "        val_grad = flatten_array(val_grad)\n",
        "        delta = (val_grad.dot(self.dFdlr)).data.cpu().numpy()\n",
        "        self.eta -= mlr * delta\n",
        "        self.eta = np.maximum(0.0, self.eta)\n",
        "\n",
        "\n",
        "class AResNet(ResNet):\n",
        "\n",
        "    def __init__(self, block, num_blocks, \\\n",
        "                num_classes=10, lr_init=0.00001, is_cuda=1):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "        self.param_shapes = [tuple(p.shape) for p in self.parameters()]\n",
        "        self.param_sizes = [p.numel() for p in self.parameters()]\n",
        "        self.param_cumsum = np.cumsum([0] + self.param_sizes)\n",
        "        self.n_params = sum(self.param_sizes)\n",
        "\n",
        "        self.reset_jacob(is_cuda)\n",
        "        self.name = 'AREZ'\n",
        "        self.grad_norm = 0\n",
        "        self.grad_norm_vl = 0\n",
        "        self.grad_angle = 0\n",
        "        self.param_norm = 0\n",
        "\n",
        "        self.eta  = np.ones(len(self.param_sizes)) * lr_init\n",
        "\n",
        "\n",
        "    def _get_adaptive_hyper(self, lambda_l2, is_cuda=0):\n",
        "\n",
        "        layerwise_eta, layerwise_l2, layerwise_eta_np, layerwise_l2_np = [], [], [], []\n",
        "        for i, shape in enumerate(self.param_shapes):\n",
        "            layerwise_eta.append(self.eta[i] * torch.ones(shape).flatten())\n",
        "            layerwise_l2.append(lambda_l2[i] * torch.ones(shape).flatten())\n",
        "\n",
        "        layerwise_l2 = torch.cat(layerwise_l2)\n",
        "        layerwise_eta = torch.cat(layerwise_eta)\n",
        "\n",
        "        if is_cuda:\n",
        "            layerwise_l2 = layerwise_l2.cuda()\n",
        "            layerwise_eta = layerwise_eta.cuda()\n",
        "        return layerwise_eta, layerwise_l2\n",
        "\n",
        "\n",
        "    def update_dFdlr(self, Hv, param, grad, lambda_l2, is_cuda=0, opt_type='sgd', noise=None, N=50000):\n",
        "        layerwise_eta, layerwise_l2 = self._get_adaptive_hyper(lambda_l2, is_cuda)\n",
        "\n",
        "        self.Hlr = layerwise_eta *Hv\n",
        "        self.Hlr_norm = norm(self.Hlr)\n",
        "        self.dFdlr_norm = norm(self.dFdlr)\n",
        "        self.dFdlr.data = self.dFdlr.data * (1-2*layerwise_l2*layerwise_eta) \\\n",
        "                                - self.Hlr - grad - 2*layerwise_l2*param\n",
        "        if opt_type == 'sgld':\n",
        "            if noise is None: noise = torch.randn(size=param.shape)\n",
        "            self.dFdlr.data = self.dFdlr.data +  0.5 * torch.sqrt(2 * noise  / N / layerwise_eta)\n",
        "\n",
        "\n",
        "    def update_eta(self, mlr, val_grad):\n",
        "\n",
        "        dFdlr_ = unflatten_array(self.dFdlr, self.param_cumsum, self.param_shapes)\n",
        "        for i, (dFdlr_l, val_grad_l) in enumerate(zip(dFdlr_, val_grad)):\n",
        "            dFdlr_l = flatten_array(dFdlr_l)\n",
        "            val_grad_l = flatten_array(val_grad_l)\n",
        "            delta = (val_grad_l.dot(dFdlr_l)).data.cpu().numpy()\n",
        "            self.eta[i] -= mlr * delta\n",
        "            self.eta[i] = np.maximum(0, self.eta[i])\n",
        "\n",
        "\n",
        "\n",
        "def AResNet18(lr_init):\n",
        "    return AResNet(BasicBlock, [2, 2, 2, 2], lr_init=lr_init)\n",
        "\n",
        "\n",
        "def model_patched(model, param_tensors, x):\n",
        "    idx = 0\n",
        "\n",
        "    def conv_bn_relu(x, weight, bn_weight, bn_bias, running_mean, running_var):\n",
        "        x = F.conv2d(x, weight, bias=None, stride=1, padding=1)\n",
        "        x = F.batch_norm(x, running_mean.detach(), running_var.detach(), bn_weight, bn_bias, training=False)\n",
        "        return F.relu(x)\n",
        "\n",
        "    def basic_block(x, weights, bns, shortcut_weights=None, shortcut_bns=None, stride=1):\n",
        "        out = F.conv2d(x, weights[0], bias=None, stride=stride, padding=1)\n",
        "        out = F.batch_norm(out, bns[0][0].detach(), bns[0][1].detach(), bns[0][2], bns[0][3], training=False)\n",
        "        out = F.relu(out)\n",
        "        out = F.conv2d(out, weights[1], bias=None, stride=1, padding=1)\n",
        "        out = F.batch_norm(out, bns[1][0].detach(), bns[1][1].detach(), bns[1][2], bns[1][3], training=False)\n",
        "\n",
        "        identity = x\n",
        "        if shortcut_weights is not None:\n",
        "            identity = F.conv2d(x, shortcut_weights[0], bias=None, stride=stride)\n",
        "            identity = F.batch_norm(identity, shortcut_bns[0].detach(), shortcut_bns[1].detach(),\n",
        "                                    shortcut_bns[2], shortcut_bns[3], training=False)\n",
        "        out += identity\n",
        "        return F.relu(out)\n",
        "\n",
        "    conv1_weight = param_tensors[idx]; idx += 1\n",
        "    bn1_weight = param_tensors[idx]; idx += 1\n",
        "    bn1_bias = param_tensors[idx]; idx += 1\n",
        "    bn1_running_mean = model.bn1.running_mean.detach()\n",
        "    bn1_running_var = model.bn1.running_var.detach()\n",
        "\n",
        "    x = F.conv2d(x, conv1_weight, bias=None, stride=1, padding=1)\n",
        "    x = F.batch_norm(x, bn1_running_mean, bn1_running_var, bn1_weight, bn1_bias, training=False)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    for layer in [model.layer1, model.layer2, model.layer3, model.layer4]:\n",
        "        for block in layer:\n",
        "            conv1_w = param_tensors[idx]; idx += 1\n",
        "            bn1_w = param_tensors[idx]; idx += 1\n",
        "            bn1_b = param_tensors[idx]; idx += 1\n",
        "            conv2_w = param_tensors[idx]; idx += 1\n",
        "            bn2_w = param_tensors[idx]; idx += 1\n",
        "            bn2_b = param_tensors[idx]; idx += 1\n",
        "\n",
        "            shortcut_weights, shortcut_bns = None, None\n",
        "            if isinstance(block.shortcut, torch.nn.Sequential) and len(block.shortcut) > 0:\n",
        "                shortcut_conv_w = param_tensors[idx]; idx += 1\n",
        "                shortcut_bn_w = param_tensors[idx]; idx += 1\n",
        "                shortcut_bn_b = param_tensors[idx]; idx += 1\n",
        "                shortcut_weights = [shortcut_conv_w]\n",
        "                shortcut_bns = [block.shortcut[1].running_mean,\n",
        "                                block.shortcut[1].running_var,\n",
        "                                shortcut_bn_w,\n",
        "                                shortcut_bn_b]\n",
        "\n",
        "            x = basic_block(x,\n",
        "                            weights=[conv1_w, conv2_w],\n",
        "                            bns=[\n",
        "                                [block.bn1.running_mean, block.bn1.running_var, bn1_w, bn1_b],\n",
        "                                [block.bn2.running_mean, block.bn2.running_var, bn2_w, bn2_b]\n",
        "                            ],\n",
        "                            shortcut_weights=shortcut_weights,\n",
        "                            shortcut_bns=shortcut_bns,\n",
        "                            stride=block.conv1.stride[0])\n",
        "\n",
        "    x = F.avg_pool2d(x, 4)\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    linear_weight = param_tensors[idx]; idx += 1\n",
        "    linear_bias = param_tensors[idx]; idx += 1\n",
        "    x = F.linear(x, linear_weight, linear_bias)\n",
        "\n",
        "    return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J-n45PyJ2ww"
      },
      "outputs": [],
      "source": [
        "class SGD_Multi_LR(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=0.005):\n",
        "\n",
        "        params, params_copy = tee(params)\n",
        "        LR = []\n",
        "        for p in params:\n",
        "            LR.append(lr*np.ones(p.shape))\n",
        "\n",
        "        defaults = dict(lr=LR)\n",
        "        super(SGD_Multi_LR, self).__init__(params_copy, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD_Multi_LR, self).__setstate__(state)\n",
        "\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Performs a single optimization step.\"\"\"\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for param, lr in zip(group['params'], group['lr']):\n",
        "                if param.grad is None:\n",
        "                    continue\n",
        "\n",
        "                d_p = param.grad.data\n",
        "                lr = torch.from_numpy(np.asarray([lr]))\n",
        "\n",
        "                if d_p.is_cuda:\n",
        "                    lr = lr.cuda()\n",
        "\n",
        "                p_change = -lr[0] * (d_p)\n",
        "                param.data.add_(p_change)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss(pred, target, model):\n",
        "    return F.nll_loss(pred, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVYdz86dKLEv"
      },
      "outputs": [],
      "source": [
        "TRAIN=0\n",
        "VALID=1\n",
        "TEST =2\n",
        "\n",
        "def train(dataset, model, optimizer, mlr, opt_type, reset_freq, update_freq, model_type, gamma, step_size, num_epoch, saveF=0, is_cuda=1):\n",
        "\n",
        "    start_time0 = time.time()\n",
        "    opt_type = opt_type\n",
        "    if 'step' in opt_type:\n",
        "        lrsch_type = opt_type.split('_')[-1]\n",
        "        if 'sgd_expstep' == opt_type:\n",
        "            scheduler = lr_scheduler_init(optimizer, lrsch_type, gamma=gamma)\n",
        "        elif 'sgd_step' == opt_type:\n",
        "            scheduler = lr_scheduler_init(optimizer, lrsch_type, step_size=step_size)\n",
        "        else:\n",
        "            scheduler = lr_scheduler_init(optimizer, lrsch_type, N=num_epoch+1)\n",
        "\n",
        "    counter = 0\n",
        "    lr_list = []\n",
        "    dFdlr_list, dFdl2_list, Wn_list, gang_list = [], [], [], []\n",
        "    tr_epoch, tr_loss_list, tr_acc_list = [], [], []\n",
        "    vl_epoch, vl_loss_list, vl_acc_list = [], [], []\n",
        "    te_epoch, te_loss_list, te_acc_list = [], [], []\n",
        "    tr_corr_mean_list, tr_corr_std_list = [], []\n",
        "    optimizer = update_optimizer_hyperparams(model, optimizer)\n",
        "\n",
        "    lambda_l2_params = nn.ParameterList([\n",
        "      nn.Parameter(torch.tensor(0.0000, dtype=torch.float32))\n",
        "      for _ in range(len(model.module.param_sizes))\n",
        "    ])\n",
        "    evo_grad_opt = torch.optim.SGD(lambda_l2_params.parameters(), lr=mlr)\n",
        "\n",
        "    for epoch in range(num_epoch+1):\n",
        "        if epoch % 1 == 0:\n",
        "            te_losses, te_accs = [], []\n",
        "            for batch_idx, (data, target) in enumerate(dataset[TEST]):\n",
        "                data, target = to_torch_variable(data, target, is_cuda)\n",
        "                _, loss, accuracy, _, _, _ = feval(data, target, model, optimizer, lambda_l2_params, mode='eval', is_cuda=is_cuda)\n",
        "                te_losses.append(loss)\n",
        "                te_accs.append(accuracy)\n",
        "            te_epoch.append(epoch)\n",
        "            te_loss_list.append(np.mean(te_losses))\n",
        "            te_acc_list.append(np.mean(te_accs))\n",
        "\n",
        "            print('Valid Epoch: %d, Loss %f Acc %f' %\n",
        "                (epoch, np.mean(te_losses), np.mean(te_accs)))\n",
        "\n",
        "            if 'step' in opt_type:\n",
        "                scheduler.step()\n",
        "                model.module.eta = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        grad_list = []\n",
        "        start_time = time.time()\n",
        "        for batch_idx, (data, target) in enumerate(dataset[TRAIN]):\n",
        "            if batch_idx % 100 == 0:\n",
        "                print('Train Epoch: %d, Batch %d out of %d' % (epoch, batch_idx, len(dataset[TRAIN])))\n",
        "            data, target = to_torch_variable(data, target, is_cuda)\n",
        "            if 'step' in opt_type:\n",
        "                model, loss, accuracy, output, noise, grad_vec = feval(data, target, model, optimizer, lambda_l2_params, \\\n",
        "                                is_cuda=is_cuda, mode='train', opt_type=opt_type)\n",
        "            else:\n",
        "                model, loss, accuracy, output, noise, grad_vec = feval(data, target, model, optimizer, lambda_l2_params, \\\n",
        "                                is_cuda=is_cuda, mode='meta-train', opt_type=opt_type)\n",
        "            tr_epoch.append(counter)\n",
        "            tr_loss_list.append(loss)\n",
        "            tr_acc_list.append(accuracy)\n",
        "            if batch_idx % 5 == 0: grad_list.append(grad_vec)\n",
        "\n",
        "            if reset_freq > 0 and counter % reset_freq == 0:\n",
        "                model_ = model.module if 'DataParallel' in str(type(model)) else model\n",
        "                model_.reset_jacob(is_cuda)\n",
        "\n",
        "            if epoch % update_freq == 0 and 'step' not in opt_type and mlr != 0.0:\n",
        "                data_vl, target_vl = next(dataset[VALID])\n",
        "                data_vl, target_vl = to_torch_variable(data_vl, target_vl, is_cuda)\n",
        "\n",
        "                model, loss_vl, optimizer = meta_update(data_vl, target_vl, data, target, model, optimizer, evo_grad_opt, lambda_l2_params, mlr, noise, is_cuda=is_cuda)\n",
        "                vl_epoch.append(counter)\n",
        "                vl_loss_list.append(loss_vl.item())\n",
        "            counter += 1\n",
        "\n",
        "        corr_mean, corr_std = compute_correlation(grad_list, normF=1)\n",
        "        tr_corr_mean_list.append(corr_mean)\n",
        "        tr_corr_std_list.append(corr_std)\n",
        "\n",
        "        end_time = time.time()\n",
        "        if epoch == 0: print('Single epoch timing %f' % ((end_time-start_time) / 60))\n",
        "\n",
        "        model_ = model.module if 'DataParallel' in str(type(model)) else model\n",
        "        fprint = 'Train Epoch: %d, Tr Loss %f Vl loss %f Acc %f Eta %s, L2 %s, |dFdlr| %.2f |G| %.4f |G_vl| %.4f Gang %.3f |W| %.2f, Grad Corr %f %f'\n",
        "        print(fprint % (epoch, np.mean(tr_loss_list[-100:]), \\\n",
        "                        np.nanmean(vl_loss_list[-100:]), \\\n",
        "                        np.nanmean(tr_acc_list[-100:]), \\\n",
        "                        str(model_.eta), str([p for p in lambda_l2_params]), \\\n",
        "                        model_.dFdlr_norm,\\\n",
        "                        model_.grad_norm,  model_.grad_norm_vl, \\\n",
        "                        model_.grad_angle, model_.param_norm, corr_mean, corr_std))\n",
        "\n",
        "        Wn_list.append(model_.param_norm)\n",
        "        dFdlr_list.append(model_.dFdlr_norm)\n",
        "        if model_type == 'arez18' or model_type == 'qrez18':\n",
        "            lr_list.append(model_.eta.copy())\n",
        "        else:\n",
        "            lr_list.append(model_.eta)\n",
        "        gang_list.append(model_.grad_angle)\n",
        "\n",
        "    Wn_list = np.asarray(Wn_list)\n",
        "    lr_list = np.asarray(lr_list)\n",
        "    dFdlr_list = np.asarray(dFdlr_list)\n",
        "    tr_epoch = np.asarray(tr_epoch)\n",
        "    vl_epoch = np.asarray(vl_epoch)\n",
        "    te_epoch = np.asarray(te_epoch)\n",
        "    tr_acc_list = np.asarray(tr_acc_list)\n",
        "    te_acc_list = np.asarray(te_acc_list)\n",
        "    tr_loss_list = np.asarray(tr_loss_list)\n",
        "    vl_loss_list = np.asarray(vl_loss_list)\n",
        "    te_loss_list = np.asarray(te_loss_list)\n",
        "    gang_list = np.asarray(gang_list)\n",
        "    tr_corr_mean_list = np.asarray(tr_corr_mean_list)\n",
        "    tr_corr_std_list = np.asarray(tr_corr_std_list)\n",
        "\n",
        "    end_time0 = time.time()\n",
        "    print('Total training timing %f' % ((end_time0-start_time0) / 3600))\n",
        "\n",
        "    return Wn_list, l2_list, lr_list, dFdlr_list, dFdl2_list, gang_list, \\\n",
        "                tr_epoch, vl_epoch, te_epoch, tr_acc_list, te_acc_list, \\\n",
        "                tr_loss_list, vl_loss_list, te_loss_list, tr_corr_mean_list, tr_corr_std_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S52NNGlHKRB1"
      },
      "outputs": [],
      "source": [
        "def criterion(pred, target, model, lambda_l2_params):\n",
        "    loss = F.nll_loss(pred, target)\n",
        "    l2_penalty = sum((p**2).sum() * (lmbda / 2) for p, lmbda in zip(model.parameters(), lambda_l2_params))\n",
        "    return loss + l2_penalty\n",
        "\n",
        "def feval(data, target, model, optimizer, lambda_l2_params, mode='eval', is_cuda=1, opt_type='sgd', N=50000):\n",
        "\n",
        "    if mode == 'eval':\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = model(data)\n",
        "    else:\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "\n",
        "    loss = criterion(output, target, model, lambda_l2_params)\n",
        "    pred = output.argmax(dim=1, keepdim=True).flatten()\n",
        "    accuracy = pred.eq(target).float().mean()\n",
        "\n",
        "    grad_vec = []\n",
        "    noise = None\n",
        "    if 'train' in mode:\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        for i,param in enumerate(model.parameters()):\n",
        "            if opt_type == 'sgld':\n",
        "                noise = torch.randn(size=param.shape)\n",
        "                model_ = model.module if 'DataParallel' in str(type(model)) else model\n",
        "                if type(model_.eta) == type(np.array([])):\n",
        "                    eps = np.sqrt(model_.eta[i]*2/ N) * noise  if model_.eta[i] > 0 else 0 * noise\n",
        "                else:\n",
        "                    eps = np.sqrt(model_.eta*2/ N) * noise  if model_.eta > 0 else 0 * noise\n",
        "                eps = to_torch_variable(eps, is_cuda=is_cuda)\n",
        "                param.grad.data = param.grad.data + eps.data\n",
        "            grad_vec.append(param.grad.data.cpu().numpy().flatten())\n",
        "\n",
        "        if 'SGD_Quotient_LR' in str(optimizer):\n",
        "            optimizer.rez_step()\n",
        "        else:\n",
        "            optimizer.step()\n",
        "        grad_vec = np.hstack(grad_vec)\n",
        "        grad_vec = grad_vec / norm_np(grad_vec)\n",
        "\n",
        "    elif 'grad' in mode:\n",
        "        loss.backward()\n",
        "\n",
        "    return model, loss.item(), accuracy.item(), output, noise, grad_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAWR_7hrKVFA"
      },
      "outputs": [],
      "source": [
        "# evograd parameters\n",
        "n_model_candidates = 2\n",
        "sigma = 0.001\n",
        "temperature = 0.05\n",
        "\n",
        "def meta_update(data_vl, target_vl, data_tr, target_tr, model_, optimizer, evo_grad_opt, lambda_l2_params, mlr, noise=None, is_cuda=1):\n",
        "\n",
        "    model = model_.module if 'DataParallel' in str(type(model_)) else model_\n",
        "\n",
        "    param_shapes = model.param_shapes\n",
        "    dFdlr= unflatten_array(model.dFdlr, model.param_cumsum, param_shapes)\n",
        "    Hv_lr  = compute_HessianVectorProd(model, dFdlr, data_tr, target_tr, is_cuda=is_cuda)\n",
        "\n",
        "    model, loss_valid, grad_valid = get_grad_valid(model, data_vl, target_vl, is_cuda)\n",
        "\n",
        "    grad = flatten_array(get_grads(model.parameters(), is_cuda))\n",
        "    param = flatten_array(model.parameters())\n",
        "    model.grad_norm = norm(grad)\n",
        "    model.param_norm = norm(param)\n",
        "    grad_vl = flatten_array(grad_valid)\n",
        "    model.grad_angle = torch.dot(grad / model.grad_norm, grad_vl / model.grad_norm_vl).item()\n",
        "\n",
        "\n",
        "    model.update_dFdlr(Hv_lr, param, grad, lambda_l2_params, is_cuda, noise=noise)\n",
        "    model.update_eta(mlr, val_grad=grad_valid)\n",
        "    param = flatten_array_w_0bias(model.parameters()).data\n",
        "\n",
        "    model_parameters = [i.detach() for i in model.parameters()]\n",
        "    theta_list = [[j + sigma*torch.sign(torch.randn_like(j)) for j in model_parameters] for i in range(n_model_candidates)]\n",
        "    pred_list = [model_patched(model, theta, data_vl) for theta in theta_list]\n",
        "    loss_list = [criterion(pred, target_vl, model, lambda_l2_params) for pred in pred_list]\n",
        "    weights = torch.softmax(-torch.stack(loss_list) / temperature, 0)\n",
        "    theta_updated = [sum(map(mul, theta, weights)) for theta in zip(*theta_list)]\n",
        "    preds_meta = model_patched(model, theta_updated, data_vl)\n",
        "    loss_l2 = loss(preds_meta, target_vl, model)\n",
        "    evo_grad_opt.zero_grad()\n",
        "    grads = torch.autograd.grad(loss_l2, lambda_l2_params.parameters(), retain_graph=True)\n",
        "    for p, g in zip(lambda_l2_params.parameters(), grads):\n",
        "        p.grad = g\n",
        "    evo_grad_opt.step()\n",
        "    with torch.no_grad():\n",
        "      for p in lambda_l2_params:\n",
        "          p.clamp_(min=0.0, max=0.0002)\n",
        "\n",
        "    optimizer = update_optimizer_hyperparams(model, optimizer)\n",
        "\n",
        "    return model_, loss_valid, optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5XMA4ZCRKYi1"
      },
      "outputs": [],
      "source": [
        "def get_grad_valid(model, data, target, is_cuda):\n",
        "\n",
        "    val_model = deepcopy(model)\n",
        "    val_model.train()\n",
        "\n",
        "    output = val_model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    grads = get_grads(val_model.parameters(), is_cuda)\n",
        "    model.grad_norm_vl = norm(flatten_array(grads))\n",
        "\n",
        "    return model, loss, grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFb2bDE6KbbY"
      },
      "outputs": [],
      "source": [
        "def update_optimizer_hyperparams(model, optimizer):\n",
        "\n",
        "    model_ = model.module if 'DataParallel' in str(type(model)) else model\n",
        "    optimizer.param_groups[0]['lr'] = np.copy(model_.eta)\n",
        "\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFpQBHD9Kd2C",
        "outputId": "4df60647-b785-4a99-da7f-ce7f120a8f53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Building model..\n",
            "Valid Epoch: 0, Loss 2.302894 Acc 0.095900\n",
            "Train Epoch: 0, Batch 0 out of 400\n",
            "Train Epoch: 0, Batch 100 out of 400\n",
            "Train Epoch: 0, Batch 200 out of 400\n",
            "Train Epoch: 0, Batch 300 out of 400\n",
            "Single epoch timing 17.509612\n",
            "Train Epoch: 0, Tr Loss 1.437332 Vl loss 1.445358 Acc 0.461600 Eta [0.09937508 0.09999824 0.09999823 0.09935156 0.09999925 0.09999917\n",
            " 0.09949898 0.09999907 0.09999956 0.09970121 0.09999966 0.09999969\n",
            " 0.099706   0.09999952 0.09999984 0.09973374 0.09999979 0.09999983\n",
            " 0.09963909 0.09999959 0.09999979 0.09994082 0.09999974 0.09999979\n",
            " 0.09970518 0.09999985 0.09999989 0.09972945 0.09999978 0.09999992\n",
            " 0.09972575 0.09999989 0.09999991 0.09961553 0.09999975 0.09999985\n",
            " 0.09994025 0.09999983 0.09999985 0.0997635  0.09999993 0.09999994\n",
            " 0.09975645 0.09999989 0.09999992 0.09986623 0.09999997 0.09999997\n",
            " 0.09971221 0.0999997  0.09999734 0.09977108 0.09999898 0.09999734\n",
            " 0.09953579 0.09999977 0.09999988 0.09938589 0.09999925 0.09999383\n",
            " 0.09435606 0.09995991], L2 [Parameter containing:\n",
            "tensor(6.4542e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.7213e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.3056e-12, requires_grad=True), Parameter containing:\n",
            "tensor(6.3890e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.7113e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.9831e-12, requires_grad=True), Parameter containing:\n",
            "tensor(6.2661e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.7104e-08, requires_grad=True), Parameter containing:\n",
            "tensor(5.2996e-13, requires_grad=True), Parameter containing:\n",
            "tensor(6.0778e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.7112e-08, requires_grad=True), Parameter containing:\n",
            "tensor(3.6345e-13, requires_grad=True), Parameter containing:\n",
            "tensor(5.9985e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.7029e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.0184e-13, requires_grad=True), Parameter containing:\n",
            "tensor(1.1711e-08, requires_grad=True), Parameter containing:\n",
            "tensor(3.4223e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.9567e-13, requires_grad=True), Parameter containing:\n",
            "tensor(1.1819e-08, requires_grad=True), Parameter containing:\n",
            "tensor(3.4236e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.6620e-13, requires_grad=True), Parameter containing:\n",
            "tensor(1.1408e-08, requires_grad=True), Parameter containing:\n",
            "tensor(3.4284e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.6620e-13, requires_grad=True), Parameter containing:\n",
            "tensor(1.1782e-08, requires_grad=True), Parameter containing:\n",
            "tensor(3.4223e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.9641e-13, requires_grad=True), Parameter containing:\n",
            "tensor(1.1717e-08, requires_grad=True), Parameter containing:\n",
            "tensor(3.4172e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.0246e-14, requires_grad=True), Parameter containing:\n",
            "tensor(2.3166e-08, requires_grad=True), Parameter containing:\n",
            "tensor(6.8446e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.7398e-13, requires_grad=True), Parameter containing:\n",
            "tensor(2.3410e-08, requires_grad=True), Parameter containing:\n",
            "tensor(6.8420e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.5001e-13, requires_grad=True), Parameter containing:\n",
            "tensor(2.2825e-08, requires_grad=True), Parameter containing:\n",
            "tensor(6.8446e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.5001e-13, requires_grad=True), Parameter containing:\n",
            "tensor(2.3157e-08, requires_grad=True), Parameter containing:\n",
            "tensor(6.8446e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.1910e-13, requires_grad=True), Parameter containing:\n",
            "tensor(2.3254e-08, requires_grad=True), Parameter containing:\n",
            "tensor(6.8454e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.6715e-14, requires_grad=True), Parameter containing:\n",
            "tensor(4.5856e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.3689e-07, requires_grad=True), Parameter containing:\n",
            "tensor(7.9019e-14, requires_grad=True), Parameter containing:\n",
            "tensor(4.6165e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.3621e-07, requires_grad=True), Parameter containing:\n",
            "tensor(7.3743e-12, requires_grad=True), Parameter containing:\n",
            "tensor(4.5819e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.3614e-07, requires_grad=True), Parameter containing:\n",
            "tensor(7.3743e-12, requires_grad=True), Parameter containing:\n",
            "tensor(4.6276e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.3689e-07, requires_grad=True), Parameter containing:\n",
            "tensor(1.6827e-13, requires_grad=True), Parameter containing:\n",
            "tensor(4.6362e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.3555e-07, requires_grad=True), Parameter containing:\n",
            "tensor(2.1474e-11, requires_grad=True), Parameter containing:\n",
            "tensor(1.4066e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.6564e-12, requires_grad=True)], |dFdlr| 0.00 |G| 2.6470 |G_vl| 1.4897 Gang -0.428 |W| 80.18, Grad Corr 0.000382 0.115320\n",
            "Valid Epoch: 1, Loss 1.446428 Acc 0.464900\n",
            "Train Epoch: 1, Batch 0 out of 400\n",
            "Train Epoch: 1, Batch 100 out of 400\n",
            "Train Epoch: 1, Batch 200 out of 400\n",
            "Train Epoch: 1, Batch 300 out of 400\n",
            "Train Epoch: 1, Tr Loss 1.091962 Vl loss 1.074341 Acc 0.606300 Eta [0.09846096 0.09999633 0.09999637 0.09874266 0.09999843 0.0999982\n",
            " 0.0990509  0.09999752 0.09999889 0.09940488 0.09999917 0.09999924\n",
            " 0.09944977 0.09999868 0.09999962 0.09946848 0.09999948 0.09999959\n",
            " 0.09925751 0.09999892 0.09999948 0.09987608 0.09999935 0.09999948\n",
            " 0.09935515 0.09999958 0.09999973 0.0994393  0.09999943 0.09999981\n",
            " 0.09943425 0.09999969 0.0999998  0.09922818 0.0999993  0.09999969\n",
            " 0.09988152 0.09999961 0.09999969 0.09948054 0.0999998  0.09999987\n",
            " 0.09950352 0.09999971 0.09999985 0.09971062 0.09999992 0.09999993\n",
            " 0.09946393 0.09999936 0.09999447 0.09956523 0.09999749 0.09999447\n",
            " 0.09909242 0.09999915 0.09999966 0.0989264  0.09999843 0.09998696\n",
            " 0.09133823 0.09993337], L2 [Parameter containing:\n",
            "tensor(5.9907e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.4332e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.9155e-11, requires_grad=True), Parameter containing:\n",
            "tensor(5.8357e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.4251e-08, requires_grad=True), Parameter containing:\n",
            "tensor(6.8586e-12, requires_grad=True), Parameter containing:\n",
            "tensor(5.6437e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.4293e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.5699e-12, requires_grad=True), Parameter containing:\n",
            "tensor(5.3439e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.4260e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.1038e-12, requires_grad=True), Parameter containing:\n",
            "tensor(5.2559e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.4150e-08, requires_grad=True), Parameter containing:\n",
            "tensor(5.3481e-13, requires_grad=True), Parameter containing:\n",
            "tensor(1.0047e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.8520e-08, requires_grad=True), Parameter containing:\n",
            "tensor(5.8528e-13, requires_grad=True), Parameter containing:\n",
            "tensor(1.0287e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.8521e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.6434e-13, requires_grad=True), Parameter containing:\n",
            "tensor(9.5784e-09, requires_grad=True), Parameter containing:\n",
            "tensor(2.8572e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.6434e-13, requires_grad=True), Parameter containing:\n",
            "tensor(1.0210e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.8521e-08, requires_grad=True), Parameter containing:\n",
            "tensor(6.2702e-13, requires_grad=True), Parameter containing:\n",
            "tensor(1.0096e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.8445e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.5558e-13, requires_grad=True), Parameter containing:\n",
            "tensor(1.9684e-08, requires_grad=True), Parameter containing:\n",
            "tensor(5.7041e-08, requires_grad=True), Parameter containing:\n",
            "tensor(3.8516e-13, requires_grad=True), Parameter containing:\n",
            "tensor(2.0015e-08, requires_grad=True), Parameter containing:\n",
            "tensor(5.7102e-08, requires_grad=True), Parameter containing:\n",
            "tensor(3.2666e-13, requires_grad=True), Parameter containing:\n",
            "tensor(1.9109e-08, requires_grad=True), Parameter containing:\n",
            "tensor(5.6987e-08, requires_grad=True), Parameter containing:\n",
            "tensor(3.2666e-13, requires_grad=True), Parameter containing:\n",
            "tensor(1.9637e-08, requires_grad=True), Parameter containing:\n",
            "tensor(5.7042e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.8756e-13, requires_grad=True), Parameter containing:\n",
            "tensor(1.9661e-08, requires_grad=True), Parameter containing:\n",
            "tensor(5.7034e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.1289e-13, requires_grad=True), Parameter containing:\n",
            "tensor(3.8459e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.1407e-07, requires_grad=True), Parameter containing:\n",
            "tensor(1.5405e-13, requires_grad=True), Parameter containing:\n",
            "tensor(3.8776e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.1329e-07, requires_grad=True), Parameter containing:\n",
            "tensor(1.3807e-11, requires_grad=True), Parameter containing:\n",
            "tensor(3.8369e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.1331e-07, requires_grad=True), Parameter containing:\n",
            "tensor(1.3807e-11, requires_grad=True), Parameter containing:\n",
            "tensor(3.9016e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.1408e-07, requires_grad=True), Parameter containing:\n",
            "tensor(4.6539e-13, requires_grad=True), Parameter containing:\n",
            "tensor(3.8983e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.1256e-07, requires_grad=True), Parameter containing:\n",
            "tensor(3.9152e-11, requires_grad=True), Parameter containing:\n",
            "tensor(1.5961e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.7417e-12, requires_grad=True)], |dFdlr| 0.00 |G| 3.0325 |G_vl| 1.6429 Gang -0.336 |W| 80.41, Grad Corr -0.000800 0.090676\n",
            "Valid Epoch: 2, Loss 1.182340 Acc 0.598600\n",
            "Train Epoch: 2, Batch 0 out of 400\n",
            "Train Epoch: 2, Batch 100 out of 400\n",
            "Train Epoch: 2, Batch 200 out of 400\n",
            "Train Epoch: 2, Batch 300 out of 400\n",
            "Train Epoch: 2, Tr Loss 0.861345 Vl loss 0.859339 Acc 0.697000 Eta [0.09792625 0.09999502 0.09999505 0.09837528 0.09999777 0.09999743\n",
            " 0.098768   0.09999655 0.09999842 0.09919293 0.09999878 0.0999989\n",
            " 0.09927652 0.09999808 0.09999942 0.099282   0.09999918 0.09999939\n",
            " 0.09896423 0.09999828 0.09999919 0.09982628 0.09999899 0.09999919\n",
            " 0.09908398 0.09999931 0.09999958 0.09922748 0.09999914 0.09999972\n",
            " 0.09920848 0.09999948 0.0999997  0.09892667 0.09999885 0.09999954\n",
            " 0.09983439 0.09999942 0.09999954 0.09923872 0.09999968 0.09999981\n",
            " 0.09930505 0.09999956 0.09999978 0.09958171 0.09999987 0.09999991\n",
            " 0.09926831 0.09999895 0.09999235 0.0994179  0.09999629 0.09999235\n",
            " 0.09876305 0.09999849 0.09999944 0.09859695 0.09999771 0.09998191\n",
            " 0.08949273 0.09991747], L2 [Parameter containing:\n",
            "tensor(1.1039e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.3794e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.0062e-11, requires_grad=True), Parameter containing:\n",
            "tensor(1.0624e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.3819e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.9668e-11, requires_grad=True), Parameter containing:\n",
            "tensor(1.0194e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.4031e-08, requires_grad=True), Parameter containing:\n",
            "tensor(6.5913e-12, requires_grad=True), Parameter containing:\n",
            "tensor(9.4662e-09, requires_grad=True), Parameter containing:\n",
            "tensor(2.3814e-08, requires_grad=True), Parameter containing:\n",
            "tensor(5.0025e-12, requires_grad=True), Parameter containing:\n",
            "tensor(9.2703e-09, requires_grad=True), Parameter containing:\n",
            "tensor(2.3587e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.8177e-12, requires_grad=True), Parameter containing:\n",
            "tensor(1.7369e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.7662e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.1947e-12, requires_grad=True), Parameter containing:\n",
            "tensor(1.8085e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.7637e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.0749e-12, requires_grad=True), Parameter containing:\n",
            "tensor(1.6150e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.7810e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.0749e-12, requires_grad=True), Parameter containing:\n",
            "tensor(1.7831e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.7658e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.9254e-12, requires_grad=True), Parameter containing:\n",
            "tensor(1.7553e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.7546e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.0866e-12, requires_grad=True), Parameter containing:\n",
            "tensor(3.3673e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.5315e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.5459e-12, requires_grad=True), Parameter containing:\n",
            "tensor(3.4604e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.5471e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.3699e-12, requires_grad=True), Parameter containing:\n",
            "tensor(3.2063e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.5178e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.3699e-12, requires_grad=True), Parameter containing:\n",
            "tensor(3.3598e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.5316e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.2371e-12, requires_grad=True), Parameter containing:\n",
            "tensor(3.3520e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.5362e-08, requires_grad=True), Parameter containing:\n",
            "tensor(8.5555e-13, requires_grad=True), Parameter containing:\n",
            "tensor(6.4696e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.9064e-07, requires_grad=True), Parameter containing:\n",
            "tensor(4.8515e-13, requires_grad=True), Parameter containing:\n",
            "tensor(6.5588e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.8897e-07, requires_grad=True), Parameter containing:\n",
            "tensor(4.1350e-11, requires_grad=True), Parameter containing:\n",
            "tensor(6.4454e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.8876e-07, requires_grad=True), Parameter containing:\n",
            "tensor(4.1350e-11, requires_grad=True), Parameter containing:\n",
            "tensor(6.6130e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.9063e-07, requires_grad=True), Parameter containing:\n",
            "tensor(1.9138e-12, requires_grad=True), Parameter containing:\n",
            "tensor(6.5711e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.8754e-07, requires_grad=True), Parameter containing:\n",
            "tensor(1.1198e-10, requires_grad=True), Parameter containing:\n",
            "tensor(3.2940e-09, requires_grad=True), Parameter containing:\n",
            "tensor(2.6907e-12, requires_grad=True)], |dFdlr| 0.00 |G| 2.7781 |G_vl| 1.4519 Gang -0.242 |W| 80.63, Grad Corr -0.000735 0.070665\n",
            "Valid Epoch: 3, Loss 0.992077 Acc 0.655500\n",
            "Train Epoch: 3, Batch 0 out of 400\n",
            "Train Epoch: 3, Batch 100 out of 400\n",
            "Train Epoch: 3, Batch 200 out of 400\n",
            "Train Epoch: 3, Batch 300 out of 400\n",
            "Train Epoch: 3, Tr Loss 0.732780 Vl loss 0.713010 Acc 0.747200 Eta [0.0975556  0.09999401 0.09999405 0.09812996 0.0999973  0.09999686\n",
            " 0.09857333 0.09999584 0.09999807 0.09903279 0.09999846 0.09999863\n",
            " 0.09914633 0.09999762 0.09999927 0.09914052 0.0999989  0.09999922\n",
            " 0.09873867 0.09999768 0.09999894 0.09978517 0.09999865 0.09999894\n",
            " 0.09886398 0.09999905 0.09999944 0.09905511 0.09999884 0.09999963\n",
            " 0.09901435 0.09999925 0.09999958 0.09865538 0.0999984  0.09999939\n",
            " 0.09979029 0.09999923 0.09999939 0.0990086  0.09999955 0.09999975\n",
            " 0.09913027 0.0999994  0.09999973 0.09946297 0.09999982 0.09999988\n",
            " 0.09909445 0.0999985  0.09999075 0.09930425 0.09999533 0.09999075\n",
            " 0.09849492 0.09999793 0.09999925 0.09834946 0.09999708 0.09997817\n",
            " 0.08813375 0.09990649], L2 [Parameter containing:\n",
            "tensor(2.2297e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.6059e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.4913e-10, requires_grad=True), Parameter containing:\n",
            "tensor(2.1386e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.6215e-08, requires_grad=True), Parameter containing:\n",
            "tensor(8.2127e-11, requires_grad=True), Parameter containing:\n",
            "tensor(2.0512e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.6798e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.9565e-11, requires_grad=True), Parameter containing:\n",
            "tensor(1.8896e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.6250e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.5213e-11, requires_grad=True), Parameter containing:\n",
            "tensor(1.8449e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.5759e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.5871e-12, requires_grad=True), Parameter containing:\n",
            "tensor(3.4275e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.2537e-08, requires_grad=True), Parameter containing:\n",
            "tensor(6.0115e-12, requires_grad=True), Parameter containing:\n",
            "tensor(3.5983e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.2501e-08, requires_grad=True), Parameter containing:\n",
            "tensor(6.2599e-12, requires_grad=True), Parameter containing:\n",
            "tensor(3.1493e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.2749e-08, requires_grad=True), Parameter containing:\n",
            "tensor(6.2599e-12, requires_grad=True), Parameter containing:\n",
            "tensor(3.5419e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.2517e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.1928e-12, requires_grad=True), Parameter containing:\n",
            "tensor(3.4771e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.2371e-08, requires_grad=True), Parameter containing:\n",
            "tensor(3.3032e-12, requires_grad=True), Parameter containing:\n",
            "tensor(6.6232e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.8506e-07, requires_grad=True), Parameter containing:\n",
            "tensor(4.9613e-12, requires_grad=True), Parameter containing:\n",
            "tensor(6.8430e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.8540e-07, requires_grad=True), Parameter containing:\n",
            "tensor(4.4175e-12, requires_grad=True), Parameter containing:\n",
            "tensor(6.2409e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.8477e-07, requires_grad=True), Parameter containing:\n",
            "tensor(4.4175e-12, requires_grad=True), Parameter containing:\n",
            "tensor(6.6131e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.8508e-07, requires_grad=True), Parameter containing:\n",
            "tensor(3.9490e-12, requires_grad=True), Parameter containing:\n",
            "tensor(6.5812e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.8516e-07, requires_grad=True), Parameter containing:\n",
            "tensor(2.4884e-12, requires_grad=True), Parameter containing:\n",
            "tensor(1.2620e-07, requires_grad=True), Parameter containing:\n",
            "tensor(3.7013e-07, requires_grad=True), Parameter containing:\n",
            "tensor(1.3458e-12, requires_grad=True), Parameter containing:\n",
            "tensor(1.2808e-07, requires_grad=True), Parameter containing:\n",
            "tensor(3.6664e-07, requires_grad=True), Parameter containing:\n",
            "tensor(1.0128e-10, requires_grad=True), Parameter containing:\n",
            "tensor(1.2545e-07, requires_grad=True), Parameter containing:\n",
            "tensor(3.6653e-07, requires_grad=True), Parameter containing:\n",
            "tensor(1.0128e-10, requires_grad=True), Parameter containing:\n",
            "tensor(1.2922e-07, requires_grad=True), Parameter containing:\n",
            "tensor(3.7009e-07, requires_grad=True), Parameter containing:\n",
            "tensor(5.4218e-12, requires_grad=True), Parameter containing:\n",
            "tensor(1.2815e-07, requires_grad=True), Parameter containing:\n",
            "tensor(3.6367e-07, requires_grad=True), Parameter containing:\n",
            "tensor(2.6227e-10, requires_grad=True), Parameter containing:\n",
            "tensor(6.8653e-09, requires_grad=True), Parameter containing:\n",
            "tensor(6.3260e-12, requires_grad=True)], |dFdlr| 0.00 |G| 3.0926 |G_vl| 1.5833 Gang -0.423 |W| 80.86, Grad Corr -0.001122 0.058954\n",
            "Valid Epoch: 4, Loss 0.796527 Acc 0.724100\n",
            "Train Epoch: 4, Batch 0 out of 400\n",
            "Train Epoch: 4, Batch 100 out of 400\n",
            "Train Epoch: 4, Batch 200 out of 400\n",
            "Train Epoch: 4, Batch 300 out of 400\n",
            "Train Epoch: 4, Tr Loss 0.641838 Vl loss 0.649556 Acc 0.777300 Eta [0.0972908  0.09999317 0.09999323 0.09795434 0.09999684 0.09999635\n",
            " 0.09843102 0.09999534 0.09999782 0.098901   0.09999816 0.09999838\n",
            " 0.09904335 0.09999727 0.09999914 0.09903049 0.09999863 0.09999907\n",
            " 0.09855587 0.09999716 0.09999872 0.09975096 0.09999833 0.09999872\n",
            " 0.09868378 0.09999878 0.09999931 0.09891278 0.09999857 0.09999954\n",
            " 0.09885168 0.09999901 0.09999947 0.09842654 0.09999799 0.09999925\n",
            " 0.09975217 0.09999906 0.09999925 0.09880473 0.09999942 0.09999968\n",
            " 0.09897055 0.09999926 0.09999967 0.09934885 0.09999977 0.09999985\n",
            " 0.09892627 0.09999797 0.09998939 0.09920961 0.09999446 0.09998939\n",
            " 0.09825834 0.0999974  0.09999907 0.09813755 0.09999648 0.09997508\n",
            " 0.08697442 0.09989755], L2 [Parameter containing:\n",
            "tensor(2.2292e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.5265e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.7041e-10, requires_grad=True), Parameter containing:\n",
            "tensor(2.1373e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.5457e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.1161e-11, requires_grad=True), Parameter containing:\n",
            "tensor(2.0502e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.6132e-08, requires_grad=True), Parameter containing:\n",
            "tensor(2.1902e-11, requires_grad=True), Parameter containing:\n",
            "tensor(1.8852e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.5531e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.9145e-11, requires_grad=True), Parameter containing:\n",
            "tensor(1.8379e-08, requires_grad=True), Parameter containing:\n",
            "tensor(4.5058e-08, requires_grad=True), Parameter containing:\n",
            "tensor(5.1074e-12, requires_grad=True), Parameter containing:\n",
            "tensor(3.4023e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.1109e-08, requires_grad=True), Parameter containing:\n",
            "tensor(7.1611e-12, requires_grad=True), Parameter containing:\n",
            "tensor(3.5832e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.1093e-08, requires_grad=True), Parameter containing:\n",
            "tensor(7.2806e-12, requires_grad=True), Parameter containing:\n",
            "tensor(3.1085e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.1261e-08, requires_grad=True), Parameter containing:\n",
            "tensor(7.2806e-12, requires_grad=True), Parameter containing:\n",
            "tensor(3.5250e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.1062e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.2025e-11, requires_grad=True), Parameter containing:\n",
            "tensor(3.4565e-08, requires_grad=True), Parameter containing:\n",
            "tensor(9.0974e-08, requires_grad=True), Parameter containing:\n",
            "tensor(3.8573e-12, requires_grad=True), Parameter containing:\n",
            "tensor(6.5611e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.8217e-07, requires_grad=True), Parameter containing:\n",
            "tensor(6.2438e-12, requires_grad=True), Parameter containing:\n",
            "tensor(6.7963e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.8255e-07, requires_grad=True), Parameter containing:\n",
            "tensor(5.9628e-12, requires_grad=True), Parameter containing:\n",
            "tensor(6.1531e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.8173e-07, requires_grad=True), Parameter containing:\n",
            "tensor(5.9628e-12, requires_grad=True), Parameter containing:\n",
            "tensor(6.5563e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.8223e-07, requires_grad=True), Parameter containing:\n",
            "tensor(5.5029e-12, requires_grad=True), Parameter containing:\n",
            "tensor(6.5177e-08, requires_grad=True), Parameter containing:\n",
            "tensor(1.8235e-07, requires_grad=True), Parameter containing:\n",
            "tensor(2.9900e-12, requires_grad=True), Parameter containing:\n",
            "tensor(1.2455e-07, requires_grad=True), Parameter containing:\n",
            "tensor(3.6439e-07, requires_grad=True), Parameter containing:\n",
            "tensor(1.5845e-12, requires_grad=True), Parameter containing:\n",
            "tensor(1.2643e-07, requires_grad=True), Parameter containing:\n",
            "tensor(3.6069e-07, requires_grad=True), Parameter containing:\n",
            "tensor(1.1035e-10, requires_grad=True), Parameter containing:\n",
            "tensor(1.2374e-07, requires_grad=True), Parameter containing:\n",
            "tensor(3.6084e-07, requires_grad=True), Parameter containing:\n",
            "tensor(1.1035e-10, requires_grad=True), Parameter containing:\n",
            "tensor(1.2760e-07, requires_grad=True), Parameter containing:\n",
            "tensor(3.6431e-07, requires_grad=True), Parameter containing:\n",
            "tensor(6.2433e-12, requires_grad=True), Parameter containing:\n",
            "tensor(1.2637e-07, requires_grad=True), Parameter containing:\n",
            "tensor(3.5786e-07, requires_grad=True), Parameter containing:\n",
            "tensor(2.7828e-10, requires_grad=True), Parameter containing:\n",
            "tensor(6.9056e-09, requires_grad=True), Parameter containing:\n",
            "tensor(6.4828e-12, requires_grad=True)], |dFdlr| 0.00 |G| 2.8198 |G_vl| 1.4371 Gang -0.211 |W| 81.07, Grad Corr 0.000423 0.056273\n",
            "Valid Epoch: 5, Loss 0.756187 Acc 0.743700\n",
            "Train Epoch: 5, Batch 0 out of 400\n",
            "Train Epoch: 5, Batch 100 out of 400\n",
            "Train Epoch: 5, Batch 200 out of 400\n",
            "Train Epoch: 5, Batch 300 out of 400\n"
          ]
        }
      ],
      "source": [
        "dataset = load_cifar10(args)\n",
        "\n",
        "lr = 0.1\n",
        "is_cuda = 1\n",
        "mlr = 0.00001\n",
        "opt_type = 'sgd'\n",
        "reset_freq = 1\n",
        "upd_freq = 1\n",
        "model_type = 'arez18'\n",
        "gamma = 0.97\n",
        "step_size = 1000\n",
        "num_epoch = 20\n",
        "device = 'cuda'\n",
        "\n",
        "print('==> Building model..')\n",
        "model = AResNet18(lr)\n",
        "optimizer = SGD_Multi_LR(model.parameters(), lr=lr)\n",
        "\n",
        "optimizer = update_optimizer_hyperparams(model, optimizer)\n",
        "\n",
        "\n",
        "if is_cuda:\n",
        "    model = model.to(device)\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "Wn_list, l2_list, lr_list, dFdlr_list, dFdl2_list, gang_list, tr_epoch, vl_epoch, te_epoch,\\\n",
        "                            tr_acc_list, te_acc_list, \\\n",
        "                            tr_loss_list, vl_loss_list, te_loss_list, \\\n",
        "                            tr_corr_mean_list, tr_corr_std_list \\\n",
        "                            = train(dataset, model, optimizer, mlr, opt_type, reset_freq, upd_freq, model_type, gamma, step_size, num_epoch, is_cuda=is_cuda)\n",
        "\n",
        "print('Final test loss %f' % te_loss_list[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPYV1nTNpcjr",
        "outputId": "974c148c-52ec-4d1a-de30-5f879c4fb0fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Type: arez18 Opt Type: sgd meta-lr 0.000010 lr 0.100000 l2 0.000000, Update Freq 1 Reset Freq 0 |Nvl| 10000 Epoch 4\n",
            "==> Building model..\n",
            "../../save_dir/exp/cifar10/mlr0.000010_lr0.100000_l20.000000/arez18_4epoch_1000vlbz_sgd_1updatefreq_0resetfreq_1updatelabmda_fold0/\n",
            "Valid Epoch: 0, Loss 2.302152 Acc 0.111600\n",
            "Single epoch timing 11.966960\n",
            "Train Epoch: 0, Tr Loss 1.426281 Vl loss 1.406026 Acc 0.475200 Eta [0.09988647 0.10002322 0.1000125  0.10285077 0.09997592 0.09998719\n",
            " 0.09955115 0.10000604 0.10000124 0.10069115 0.10000653 0.10000471\n",
            " 0.09969797 0.09999151 0.10000315 0.0998326  0.10000406 0.10000134\n",
            " 0.10045404 0.10000608 0.09999487 0.10008082 0.10001068 0.09999487\n",
            " 0.10049646 0.1000002  0.10000003 0.09993367 0.10000161 0.0999985\n",
            " 0.10007716 0.0999981  0.10000013 0.10025845 0.09999928 0.09999958\n",
            " 0.09974884 0.10000186 0.09999958 0.09891383 0.09999818 0.09999899\n",
            " 0.10011662 0.10000145 0.09999988 0.10019482 0.0999999  0.09999965\n",
            " 0.1010579  0.1000208  0.10003459 0.10049614 0.10001233 0.10003459\n",
            " 0.1037012  0.09999842 0.10000088 0.09978266 0.10002408 0.10009914\n",
            " 0.0931067  0.09994082], L2 [1.60656929e-04 0.00000000e+00 1.87047123e-05 6.65267776e-05\n",
            " 6.15103589e-05 1.84890966e-05 2.00000000e-04 8.96638773e-05\n",
            " 8.28146819e-06 2.00000000e-04 8.85497763e-06 1.39539668e-06\n",
            " 1.70099496e-04 1.93583887e-04 3.43542635e-06 2.77895697e-05\n",
            " 4.34115356e-06 3.90860016e-07 2.00000000e-04 7.79403748e-05\n",
            " 5.33751405e-06 1.39533163e-04 3.07718921e-06 5.33751405e-06\n",
            " 1.32880690e-04 4.74235804e-06 3.58781300e-06 1.64223462e-04\n",
            " 1.65856046e-04 1.00824197e-06 2.00000000e-04 1.42545702e-06\n",
            " 7.89553623e-07 1.61352321e-04 1.47188898e-04 1.85384716e-06\n",
            " 1.21844254e-04 3.56206695e-05 1.85384716e-06 6.77803937e-05\n",
            " 2.54676486e-07 0.00000000e+00 7.15191127e-05 1.37877006e-04\n",
            " 3.84696455e-07 2.00000000e-04 5.99053856e-08 1.79184272e-08\n",
            " 1.69596205e-04 2.00000000e-04 0.00000000e+00 1.84013793e-04\n",
            " 2.00000000e-04 0.00000000e+00 1.36318113e-04 5.14752609e-06\n",
            " 1.78572879e-06 9.67286587e-06 2.00000000e-04 0.00000000e+00\n",
            " 1.16032772e-04 9.18856490e-06], |dFdlr| 2683.59 |dFdl2| 6822.72 |G| 3.5841 |G_vl| 2.6682 Gang -0.527 |W| 79.93, Grad Corr 0.000090 0.110953\n",
            "Train Epoch: 1, Tr Loss 1.038593 Vl loss 1.055045 Acc 0.625400 Eta [0.09792531 0.10010683 0.09992999 0.10266894 0.0999762  0.09994216\n",
            " 0.09466827 0.10001289 0.10000462 0.10160998 0.10001128 0.10001951\n",
            " 0.0949177  0.09994654 0.10000898 0.09755123 0.10000909 0.1000091\n",
            " 0.10034913 0.10002584 0.09998907 0.10048432 0.10003095 0.09998907\n",
            " 0.10115905 0.10000865 0.10000204 0.10114286 0.10000283 0.09999832\n",
            " 0.09967243 0.09999786 0.09999988 0.09986593 0.09998782 0.10000411\n",
            " 0.09978999 0.1000084  0.10000411 0.09733008 0.09999507 0.09999841\n",
            " 0.09941058 0.10000312 0.10000106 0.09996216 0.09999969 0.10000009\n",
            " 0.10088362 0.10003964 0.10007939 0.09739538 0.10001797 0.10007939\n",
            " 0.10382323 0.09998627 0.10000718 0.09923494 0.10004441 0.10017337\n",
            " 0.08773378 0.09981019], L2 [1.28478623e-05 2.61225505e-05 3.39135453e-05 2.00000000e-04\n",
            " 2.00000000e-04 9.68336712e-05 3.73706516e-05 1.04354122e-04\n",
            " 3.27962424e-05 3.03351626e-05 5.11923858e-05 2.73028469e-05\n",
            " 1.38323936e-04 1.72615271e-04 2.15395367e-05 1.01648981e-04\n",
            " 2.98312434e-05 1.39361037e-06 2.00000000e-04 6.56277947e-05\n",
            " 1.36684092e-05 9.99936085e-05 1.73781266e-04 1.36684092e-05\n",
            " 1.22007077e-05 1.22803728e-05 1.08225243e-05 1.76834475e-04\n",
            " 6.16861690e-05 9.49979613e-06 1.23330410e-04 9.93494241e-07\n",
            " 2.91634468e-08 1.37361528e-04 1.95104183e-04 3.27668025e-06\n",
            " 1.95613420e-04 9.14754997e-05 3.27668025e-06 8.58300325e-05\n",
            " 1.16470990e-06 1.33219075e-06 4.14009579e-05 4.67727914e-05\n",
            " 7.38221178e-07 1.99565125e-04 6.19757741e-06 1.49203134e-06\n",
            " 3.45257663e-05 2.00000000e-04 0.00000000e+00 2.00000000e-04\n",
            " 1.58252505e-04 0.00000000e+00 2.94451420e-05 3.03884016e-05\n",
            " 5.77328361e-06 5.80130577e-05 2.00000000e-04 0.00000000e+00\n",
            " 3.76404350e-05 1.40979348e-04], |dFdlr| 3789.69 |dFdl2| 13185.93 |G| 3.4582 |G_vl| 1.8741 Gang -0.350 |W| 79.91, Grad Corr 0.001459 0.085724\n",
            "Train Epoch: 2, Tr Loss 0.860299 Vl loss 0.858464 Acc 0.692800 Eta [0.09542746 0.100196   0.09985499 0.10260673 0.09996022 0.09988443\n",
            " 0.08842184 0.09995847 0.09998724 0.10310347 0.10000213 0.10010224\n",
            " 0.09113129 0.09992731 0.10001168 0.09655417 0.10000533 0.09999996\n",
            " 0.10015451 0.10005352 0.09998364 0.10117116 0.1000515  0.09998364\n",
            " 0.10158268 0.1000013  0.09998603 0.10194226 0.10001543 0.09999578\n",
            " 0.09919746 0.1000047  0.10000214 0.09941837 0.09998382 0.10001096\n",
            " 0.10043237 0.10001802 0.10001096 0.09656957 0.09999841 0.09999765\n",
            " 0.0986399  0.10000152 0.10000135 0.10041666 0.10000082 0.10000169\n",
            " 0.10025851 0.10004888 0.10012986 0.09539407 0.10002521 0.10012986\n",
            " 0.10580429 0.09997518 0.10001742 0.09992702 0.10009306 0.10024709\n",
            " 0.08515923 0.0996863 ], L2 [1.17467280e-04 1.88187318e-04 8.55793429e-06 2.00000000e-04\n",
            " 2.00000000e-04 1.99846155e-04 1.63257724e-04 0.00000000e+00\n",
            " 8.80209285e-05 1.45736123e-04 2.00000000e-04 1.15784602e-04\n",
            " 8.56317914e-05 4.82184714e-05 4.69739515e-05 1.51744186e-04\n",
            " 8.64951459e-05 1.69549066e-05 2.00000000e-04 1.83670382e-04\n",
            " 3.13841408e-05 9.28511898e-05 2.00000000e-04 3.13841408e-05\n",
            " 1.19685442e-05 4.39158102e-05 4.28832244e-05 1.60590620e-04\n",
            " 0.00000000e+00 1.66956273e-05 6.72274824e-05 8.74719920e-06\n",
            " 8.87469123e-06 2.00000000e-04 0.00000000e+00 1.77759224e-07\n",
            " 1.73543005e-05 2.00000000e-04 1.77759224e-07 4.26696663e-05\n",
            " 1.84168519e-05 8.71739917e-06 1.48089452e-04 4.03850527e-06\n",
            " 5.17759395e-07 1.77692418e-04 1.78488470e-05 5.27892809e-06\n",
            " 0.00000000e+00 2.00000000e-04 3.76635416e-06 5.28412766e-05\n",
            " 0.00000000e+00 3.76635416e-06 1.32144878e-04 7.87674547e-05\n",
            " 1.31283825e-05 0.00000000e+00 0.00000000e+00 5.00624191e-06\n",
            " 8.21563599e-05 1.69830782e-04], |dFdlr| 4489.82 |dFdl2| 19411.54 |G| 3.0172 |G_vl| 1.4093 Gang -0.100 |W| 79.87, Grad Corr -0.001541 0.065871\n",
            "Train Epoch: 3, Tr Loss 0.704215 Vl loss 0.728093 Acc 0.752000 Eta [0.09439157 0.10026153 0.09969118 0.10053006 0.09990122 0.09972692\n",
            " 0.08487083 0.09986814 0.09997556 0.10388386 0.09997688 0.10022924\n",
            " 0.08932026 0.09993859 0.10003122 0.09654603 0.09997117 0.09996354\n",
            " 0.09682125 0.10007988 0.09999672 0.10199536 0.10007095 0.09999672\n",
            " 0.1007662  0.09998656 0.09996375 0.10202698 0.10003459 0.10000935\n",
            " 0.09908898 0.1000023  0.09999938 0.09821476 0.09997798 0.10001906\n",
            " 0.10083836 0.10005314 0.10001906 0.09813053 0.09999448 0.09999691\n",
            " 0.0973615  0.10000233 0.10000461 0.10184476 0.10000159 0.10000098\n",
            " 0.1000802  0.10006743 0.10018835 0.09462749 0.10004036 0.10018835\n",
            " 0.10511593 0.09997021 0.10003198 0.0977778  0.10015018 0.10032389\n",
            " 0.0855966  0.09959044], L2 [0.00000000e+00 2.00000000e-04 0.00000000e+00 2.00000000e-04\n",
            " 2.00000000e-04 1.97308930e-04 2.00000000e-04 0.00000000e+00\n",
            " 1.53705447e-04 9.67839329e-05 2.00000000e-04 1.98142943e-04\n",
            " 1.25691173e-04 7.07790223e-05 5.46936319e-05 1.34683460e-05\n",
            " 1.90756292e-04 2.22516521e-05 0.00000000e+00 0.00000000e+00\n",
            " 4.54045589e-05 0.00000000e+00 1.92290195e-04 4.54045589e-05\n",
            " 1.51922236e-04 9.76760957e-05 8.09258262e-05 1.95223284e-04\n",
            " 2.44988096e-05 2.59581320e-05 1.56441913e-04 2.77735329e-05\n",
            " 2.08465267e-05 2.00000000e-04 0.00000000e+00 3.40575168e-06\n",
            " 1.23549230e-05 2.00000000e-04 3.40575168e-06 1.03790349e-04\n",
            " 5.47468404e-05 1.73027846e-05 1.30653880e-05 9.41337017e-05\n",
            " 3.86075552e-06 1.55447249e-04 3.53541541e-05 9.41734435e-06\n",
            " 1.46729335e-05 2.00000000e-04 1.68328259e-06 7.04710292e-05\n",
            " 9.99279018e-06 1.68328259e-06 2.53370763e-05 1.33714978e-04\n",
            " 2.89854645e-05 1.15072858e-04 2.00000000e-04 0.00000000e+00\n",
            " 1.62892655e-04 1.86125818e-04], |dFdlr| 4927.58 |dFdl2| 25543.07 |G| 2.9702 |G_vl| 1.3716 Gang -0.267 |W| 79.80, Grad Corr -0.001257 0.061631\n",
            "Train Epoch: 4, Tr Loss 0.632222 Vl loss 0.644292 Acc 0.781600 Eta [0.09478525 0.10042483 0.09959268 0.09886936 0.09982312 0.0996218\n",
            " 0.08057318 0.09974429 0.09994324 0.10796674 0.0999444  0.10039172\n",
            " 0.09022584 0.09994661 0.10004841 0.09657191 0.09997131 0.09992125\n",
            " 0.09253862 0.10010082 0.10001715 0.10233203 0.1000906  0.10001715\n",
            " 0.09995672 0.09998492 0.09995514 0.10292559 0.10003742 0.10003348\n",
            " 0.09826951 0.09999731 0.10000468 0.09689703 0.09997584 0.10002985\n",
            " 0.10153971 0.10008312 0.10002985 0.09952247 0.09998515 0.10000234\n",
            " 0.09534409 0.10000249 0.10001281 0.10175706 0.10000698 0.10000231\n",
            " 0.09948321 0.10008886 0.10024451 0.09495618 0.10006818 0.10024451\n",
            " 0.10373266 0.09996048 0.10005296 0.09364969 0.10024336 0.10041716\n",
            " 0.08477818 0.09953754], L2 [0.00000000e+00 1.62923626e-04 0.00000000e+00 1.72510525e-04\n",
            " 2.00000000e-04 1.97623222e-04 1.98133798e-04 0.00000000e+00\n",
            " 2.00000000e-04 6.37154654e-05 2.00000000e-04 1.92033738e-04\n",
            " 5.23943754e-06 2.00000000e-04 7.62084138e-05 0.00000000e+00\n",
            " 1.98048729e-04 1.44066528e-05 1.19278584e-05 1.88532274e-05\n",
            " 8.30506750e-05 2.00000000e-04 1.81185357e-04 8.30506750e-05\n",
            " 1.78697202e-04 1.99692223e-04 1.18320667e-04 1.79215909e-04\n",
            " 3.37053671e-05 4.66699231e-05 0.00000000e+00 9.03009696e-05\n",
            " 5.09878017e-05 9.10051413e-05 3.50685298e-05 5.46197198e-06\n",
            " 1.28511905e-04 1.91698877e-04 5.46197198e-06 6.82713016e-05\n",
            " 1.38115977e-04 3.35442099e-05 0.00000000e+00 2.57635529e-05\n",
            " 1.75286805e-05 1.38207463e-04 5.14610535e-05 2.02600449e-05\n",
            " 0.00000000e+00 2.00000000e-04 0.00000000e+00 1.17239587e-04\n",
            " 8.53892416e-05 0.00000000e+00 0.00000000e+00 1.71226993e-04\n",
            " 3.84307321e-05 9.17680154e-07 6.95718550e-05 0.00000000e+00\n",
            " 0.00000000e+00 1.85197217e-04], |dFdlr| 5260.80 |dFdl2| 31572.79 |G| 3.3634 |G_vl| 1.3680 Gang -0.244 |W| 79.68, Grad Corr -0.001058 0.057518\n",
            "Total training timing 0.955451\n",
            "Final test loss 2.302152\n",
            "<class 'numpy.float64'>\n"
          ]
        }
      ],
      "source": [
        "!python -u main.py --is_cuda 1 --ifold 0 --mlr 0.00001 --lr 0.1 --lambda_l2 0.0000 --opt_type sgd --update_freq 1 --save 1  --model_type arez18 --num_epoch 4 --batch_size_vl 1000 --update_lambda 1 --save_dir '../../save_dir'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
