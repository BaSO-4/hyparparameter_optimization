{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtX1o09H_T8R"
      },
      "source": [
        "<h3>This is <strong>Hyperwrap</strong>: a library for efficient hyperparameter optimization</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfZZKW887TEb"
      },
      "source": [
        "Cloning the OHO library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_1AUtsEHDCT",
        "outputId": "a8592ff7-4131-48ec-dd07-a01a9b375faf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'OHO'...\n",
            "remote: Enumerating objects: 114, done.\u001b[K\n",
            "remote: Counting objects: 100% (114/114), done.\u001b[K\n",
            "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
            "remote: Total 114 (delta 55), reused 73 (delta 29), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (114/114), 511.40 KiB | 14.21 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jiwoongim/OHO.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "756nQindTfh1",
        "outputId": "26e273a4-28d2-4852-92f3-8304355adbda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/OHO\n",
            "Obtaining file:///content/OHO\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: metaopt\n",
            "  Running setup.py develop for metaopt\n",
            "Successfully installed metaopt-1.0\n",
            "/content\n",
            "/content/OHO\n"
          ]
        }
      ],
      "source": [
        "%cd OHO\n",
        "!pip install -e .\n",
        "%cd ..\n",
        "%mkdir save_dir\n",
        "%cd OHO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRfNajbfGW_I"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdfr-D2NGM2c",
        "outputId": "cbb9be3b-cb68-48cd-af59-93e9c9218049"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'metaopt/mnist'\n",
            "/content/OHO/metaopt/mnist\n"
          ]
        }
      ],
      "source": [
        "# %cd ../..\n",
        "try: \n",
        "    from metaopt.mnist.main import load_mnist\n",
        "except:\n",
        "    %cd metaopt/mnist\n",
        "    from metaopt.mnist.main import load_mnist\n",
        "from metaopt.util import *\n",
        "from metaopt.util_ml import *\n",
        "from metaopt.mnist.mlp import *\n",
        "\n",
        "import os, sys, math, argparse, time, pickle\n",
        "import torch, torch.nn as nn, torch.optim as optim, numpy as np\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from copy import copy\n",
        "from itertools import tee\n",
        "from operator import mul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1VBnFPfGZHP"
      },
      "source": [
        "Some argument settings for using the OHO library\n",
        "Download the MNIST database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL2QRhplCR26",
        "outputId": "9d34cf67-d059-46c2-ad35-fe29b2216ea8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 479kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.46MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.47MB/s]\n"
          ]
        }
      ],
      "source": [
        "parser = argparse.ArgumentParser(description='Example parser')\n",
        "parser.add_argument('--valid_size', type=int, default=10000)\n",
        "parser.add_argument('--batch_size', type=int, default=100)\n",
        "parser.add_argument('--is_cuda', type=int)\n",
        "parser.add_argument('--batch_size_vl', type=int)\n",
        "args = parser.parse_args(['--valid_size', '10000', '--batch_size', '100', '--is_cuda', '1', '--batch_size_vl', '100'])\n",
        "dataset = load_mnist(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjnSIlS9GhPY"
      },
      "source": [
        "Model definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noIVTYqFQY5b"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers, layer_sizes, lr_init, is_cuda=0):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.n_layers = n_layers\n",
        "        self.n_params = 0\n",
        "        for i in range(1, self.n_layers):\n",
        "            attr = 'layer_{}'.format(i)\n",
        "            layer = nn.Linear(layer_sizes[i - 1], layer_sizes[i])\n",
        "            if is_cuda: layer = layer.cuda()\n",
        "            setattr(self, attr, layer)\n",
        "\n",
        "            param_size = (layer_sizes[i - 1] + 1) * layer_sizes[i]\n",
        "            self.n_params += param_size\n",
        "\n",
        "        self.param_sizes = [p.numel() for p in self.parameters()]\n",
        "        self.param_shapes = [tuple(p.shape) for p in self.parameters()]\n",
        "        self.param_cumsum = np.cumsum([0] + self.param_sizes)\n",
        "\n",
        "        self.reset_jacob(is_cuda)\n",
        "        self.eta  = lr_init\n",
        "        # self.lambda_l2 = lambda_l2\n",
        "        self.name = 'MLP'\n",
        "        self.grad_norm = 0\n",
        "        self.grad_norm_vl = 0\n",
        "        self.grad_angle = 0\n",
        "        self.param_norm = 0\n",
        "        self.dFdlr_norm = 0\n",
        "        self.dFdl2_nrom = 0\n",
        "\n",
        "    def reset_jacob(self, is_cuda=1):\n",
        "        self.dFdlr = torch.zeros(self.n_params)\n",
        "        self.dFdlr_norm = 0\n",
        "        if is_cuda:\n",
        "            self.dFdlr = self.dFdlr.cuda()\n",
        "\n",
        "    def forward(self, x, logsoftmaxF=1):\n",
        "\n",
        "        x = x.view(-1, self.layer_sizes[0])\n",
        "        for i_layer in range(1, self.n_layers):\n",
        "            attr = 'layer_{}'.format(i_layer)\n",
        "            layer = getattr(self, attr)\n",
        "            x = layer(x)\n",
        "            if i_layer < self.n_layers - 1:\n",
        "                x = torch.tanh(x)\n",
        "        if logsoftmaxF:\n",
        "            return F.log_softmax(x, dim=1)\n",
        "        else:\n",
        "            return F.softmax(x, dim=1)\n",
        "\n",
        "    def update_dFdlr(self, Hv, param, grad, is_cuda=0, opt_type='sgd', noise=None, N=50000):\n",
        "        self.Hlr = self.eta*Hv\n",
        "        self.Hlr_norm = norm(self.Hlr)\n",
        "        self.dFdlr_norm = norm(self.dFdlr)\n",
        "        if opt_type == 'sgld':\n",
        "            if noise is None: noise = torch.randn(size=param.shape)\n",
        "            self.dFdlr.data = self.dFdlr.data + 0.5 * torch.sqrt(2*noise / self.eta / N)\n",
        "        - self.Hl2  - 2*self.eta*param\n",
        "\n",
        "\n",
        "    def update_eta(self, mlr, val_grad):\n",
        "\n",
        "        val_grad = flatten_array(val_grad)\n",
        "        delta = val_grad.dot(self.dFdlr).data.cpu().numpy()\n",
        "        self.eta -= mlr * delta\n",
        "        self.eta = np.maximum(0.0, self.eta)\n",
        "\n",
        "class AMLP(MLP):\n",
        "\n",
        "    def __init__(self, n_layers, layer_sizes, lr_init, is_cuda=0):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.n_layers = n_layers\n",
        "        self.n_params = 0\n",
        "        for i in range(1, self.n_layers):\n",
        "            attr = 'layer_{}'.format(i)\n",
        "            layer = nn.Linear(layer_sizes[i - 1], layer_sizes[i])\n",
        "            if is_cuda: layer = layer.cuda()\n",
        "            setattr(self, attr, layer)\n",
        "\n",
        "            param_size = (layer_sizes[i - 1] + 1) * layer_sizes[i]\n",
        "            self.n_params += param_size\n",
        "\n",
        "        self.param_sizes = [p.numel() for p in self.parameters()]\n",
        "        self.param_shapes = [tuple(p.shape) for p in self.parameters()]\n",
        "        self.param_cumsum = np.cumsum([0] + self.param_sizes)\n",
        "\n",
        "        self.reset_jacob(is_cuda)\n",
        "        self.eta  = np.ones(len(self.param_sizes)) * lr_init\n",
        "\n",
        "        self.name = 'MLP'\n",
        "\n",
        "    def _get_adaptive_hyper(self, l2_params, is_cuda=0):\n",
        "\n",
        "        layerwise_eta, layerwise_l2, layerwise_eta_np, layerwise_l2_np = [], [], [], []\n",
        "        for i, shape in enumerate(self.param_shapes):\n",
        "            layerwise_eta.append(self.eta[i] * torch.ones(shape).flatten())\n",
        "            layerwise_l2.append(l2_params[i] * torch.ones(shape).flatten())\n",
        "\n",
        "        layerwise_l2 = torch.cat(layerwise_l2)\n",
        "        layerwise_eta = torch.cat(layerwise_eta)\n",
        "\n",
        "        if is_cuda:\n",
        "            layerwise_l2 = layerwise_l2.cuda()\n",
        "            layerwise_eta = layerwise_eta.cuda()\n",
        "        return layerwise_eta, layerwise_l2\n",
        "\n",
        "    def update_dFdlr(self, Hv, param, grad, lambda_l2_params, is_cuda=0, opt_type='sgd', noise=None, N=50000):\n",
        "\n",
        "        layerwise_eta, layerwise_l2 = self._get_adaptive_hyper(lambda_l2_params, is_cuda)\n",
        "\n",
        "        self.Hlr = layerwise_eta *Hv\n",
        "        self.Hlr_norm = norm(self.Hlr)\n",
        "        self.dFdlr_norm = norm(self.dFdlr)\n",
        "        self.dFdlr.data = self.dFdlr.data * (1-2*layerwise_l2*layerwise_eta) \\\n",
        "                                - self.Hlr - grad - 2*layerwise_l2*param\n",
        "        if opt_type == 'sgld':\n",
        "            if noise is None: noise = torch.randn(size=param.shape)\n",
        "            self.dFdlr.data = self.dFdlr.data +  0.5 * torch.sqrt(2 * noise  / N / layerwise_eta)\n",
        "\n",
        "\n",
        "    def update_eta(self, mlr, val_grad):\n",
        "\n",
        "        dFdlr_ = unflatten_array(self.dFdlr, self.param_cumsum, self.param_shapes)\n",
        "        for i, (dFdlr_l, val_grad_l) in enumerate(zip(dFdlr_, val_grad)):\n",
        "            dFdlr_l = flatten_array(dFdlr_l)\n",
        "            val_grad_l = flatten_array(val_grad_l)\n",
        "            delta = (val_grad_l.dot(dFdlr_l)).data.cpu().numpy()\n",
        "            self.eta[i] -= mlr * delta\n",
        "            self.eta[i] = np.maximum(0, self.eta[i])\n",
        "\n",
        "# model as a function of parameters and an input, for evograd perturbed models ouputs\n",
        "def model_patched(model, parameters, x):\n",
        "    x = x.view(-1, model.layer_sizes[0])\n",
        "    param_idx = 0\n",
        "\n",
        "    for i in range(1, model.n_layers):\n",
        "        w_shape = (model.layer_sizes[i], model.layer_sizes[i-1])\n",
        "        b_shape = (model.layer_sizes[i],)\n",
        "\n",
        "        W = parameters[param_idx].view(w_shape)\n",
        "        b = parameters[param_idx + 1].view(b_shape)\n",
        "        param_idx += 2\n",
        "\n",
        "        x = F.linear(x, W, b)\n",
        "        if i < model.n_layers - 1:\n",
        "            x = torch.tanh(x)\n",
        "\n",
        "    return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzyCMyYvIh1h"
      },
      "source": [
        "Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J1bJaeyIjbM"
      },
      "outputs": [],
      "source": [
        "class SGD_Multi_LR(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=0.005):\n",
        "\n",
        "        params, params_copy = tee(params)\n",
        "        LR = []\n",
        "        for p in params:\n",
        "            LR.append(lr*np.ones(p.shape))\n",
        "\n",
        "        defaults = dict(lr=LR)\n",
        "        super(SGD_Multi_LR, self).__init__(params_copy, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD_Multi_LR, self).__setstate__(state)\n",
        "\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Performs a single optimization step.\"\"\"\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for param, lr in zip(group['params'], group['lr']):\n",
        "                if param.grad is None:\n",
        "                    continue\n",
        "\n",
        "                d_p = param.grad.data\n",
        "                lr = torch.from_numpy(np.asarray([lr]))\n",
        "\n",
        "                if d_p.is_cuda:\n",
        "                    lr = lr.cuda()\n",
        "                p_change = -lr[0] * (d_p)\n",
        "                param.data.add_(p_change)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss(pred, target, model):\n",
        "    return F.nll_loss(pred, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCXqconZG7K_"
      },
      "source": [
        "Training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEsbLtyIHAnT"
      },
      "outputs": [],
      "source": [
        "TRAIN=0\n",
        "VALID=1\n",
        "TEST =2\n",
        "\n",
        "\n",
        "def train(model_type, mlr, num_epoch, reset_freq, update_freq, checkpoint_freq, opt_type, dataset, model, optimizer, fdir, saveF=0, is_cuda=1):\n",
        "    counter = 0\n",
        "    lr_list, l2_list = [], []\n",
        "    dFdlr_list, Wn_list, gang_list = [], [], []\n",
        "    tr_epoch, tr_loss_list, tr_acc_list = [], [], []\n",
        "    vl_epoch, vl_loss_list, vl_acc_list = [], [], []\n",
        "    te_epoch, te_loss_list, te_acc_list = [], [], []\n",
        "    tr_corr_mean_list, tr_corr_std_list = [], []\n",
        "    optimizer = update_optimizer_hyperparams(model, optimizer)\n",
        "    lambda_l2_params = nn.ParameterList([\n",
        "        nn.Parameter(torch.tensor(0.0000, dtype=torch.float32))\n",
        "        for _ in range(len(model.param_sizes))\n",
        "    ])\n",
        "    evo_grad_opt = torch.optim.SGD(lambda_l2_params.parameters(), lr=mlr)\n",
        "\n",
        "    start_time0 = time.time()\n",
        "    for epoch in range(num_epoch+1):\n",
        "        if epoch % 10 == 0:\n",
        "            te_losses, te_accs = [], []\n",
        "            for batch_idx, (data, target) in enumerate(dataset[TEST]):\n",
        "                data, target = to_torch_variable(data, target, is_cuda, floatTensorF=1)\n",
        "\n",
        "                _, loss, accuracy, _, _, _ = feval(data, target, model, optimizer, lambda_l2_params, mode='eval', is_cuda=is_cuda)\n",
        "                te_losses.append(loss)\n",
        "                te_accs.append(accuracy)\n",
        "            te_epoch.append(epoch)\n",
        "            te_loss_list.append(np.mean(te_losses))\n",
        "            te_acc_list.append(np.mean(te_accs))\n",
        "\n",
        "            print('Valid Epoch: %d, Loss %f Acc %f' %\n",
        "                (epoch, np.mean(te_losses), np.mean(te_accs)))\n",
        "\n",
        "\n",
        "        grad_list = []\n",
        "        start_time = time.time()\n",
        "        for batch_idx, (data, target) in enumerate(dataset[TRAIN]):\n",
        "\n",
        "            data, target = to_torch_variable(data, target, is_cuda)\n",
        "            opt_type = opt_type\n",
        "            model, loss, accuracy, output, noise, grad_vec = feval(data, target, model, optimizer, lambda_l2_params, \\\n",
        "                                is_cuda=is_cuda, mode='meta-train', opt_type=opt_type)\n",
        "            tr_epoch.append(counter)\n",
        "            tr_loss_list.append(loss)\n",
        "            tr_acc_list.append(accuracy)\n",
        "            grad_list.append(grad_vec)\n",
        "\n",
        "            if reset_freq > 0 and counter % reset_freq == 0:\n",
        "                model.reset_jacob()\n",
        "\n",
        "            if counter % update_freq == 0 and mlr != 0.0:\n",
        "                data_vl, target_vl = next(dataset[VALID])\n",
        "                data_vl, target_vl = to_torch_variable(data_vl, target_vl, is_cuda)\n",
        "                model, loss_vl, optimizer = meta_update(data_vl, target_vl, data, target, model, optimizer, evo_grad_opt, lambda_l2_params, noise)\n",
        "                vl_epoch.append(counter)\n",
        "                vl_loss_list.append(loss_vl.item())\n",
        "\n",
        "            counter += 1\n",
        "        corr_mean, corr_std = compute_correlation(grad_list, normF=1)\n",
        "        tr_corr_mean_list.append(corr_mean)\n",
        "        tr_corr_std_list.append(corr_std)\n",
        "        grad_list = np.asarray(grad_list)\n",
        "\n",
        "        end_time = time.time()\n",
        "        if epoch == 0: print('Single epoch timing %f' % ((end_time-start_time) / 60))\n",
        "\n",
        "\n",
        "        if epoch % checkpoint_freq == 0:\n",
        "            os.makedirs(fdir+ '/checkpoint/', exist_ok=True)\n",
        "            save(model, fdir+ '/checkpoint/epoch%d' % epoch)\n",
        "\n",
        "\n",
        "        fprint = 'Train Epoch: %d, Tr Loss %f Vl loss %f Acc %f Eta %s, L2 %s, |dFdlr| %.2f |G| %.4f |G_vl| %.4f Gang %.3f |W| %.2f, Grad Corr %f %f'\n",
        "        print(fprint % (epoch, np.mean(tr_loss_list[-100:]), \\\n",
        "                        np.mean(vl_loss_list[-100:]), \\\n",
        "                        np.mean(tr_acc_list[-100:]), \\\n",
        "                        str(model.eta), str([p for p in lambda_l2_params]), \\\n",
        "                        model.dFdlr_norm,\\\n",
        "                        model.grad_norm,  model.grad_norm_vl, \\\n",
        "                        model.grad_angle, model.param_norm, corr_mean, corr_std))\n",
        "\n",
        "        Wn_list.append(model.param_norm)\n",
        "        dFdlr_list.append(model.dFdlr_norm)\n",
        "        if model_type == 'amlp':\n",
        "            lr_list.append(model.eta.copy())\n",
        "            l2_list.append([p.data.cpu().numpy() for p in lambda_l2_params])\n",
        "        else:\n",
        "            lr_list.append(model.eta)\n",
        "            l2_list.append(lambda_l2_params)\n",
        "        gang_list.append(model.grad_angle)\n",
        "\n",
        "    Wn_list = np.asarray(Wn_list)\n",
        "    l2_list = np.asarray(l2_list)\n",
        "    lr_list = np.asarray(lr_list)\n",
        "    dFdlr_list = np.asarray(dFdlr_list)\n",
        "    tr_epoch = np.asarray(tr_epoch)\n",
        "    vl_epoch = np.asarray(vl_epoch)\n",
        "    te_epoch = np.asarray(te_epoch)\n",
        "    tr_acc_list = np.asarray(tr_acc_list)\n",
        "    te_acc_list = np.asarray(te_acc_list)\n",
        "    tr_loss_list = np.asarray(tr_loss_list)\n",
        "    vl_loss_list = np.asarray(vl_loss_list)\n",
        "    te_loss_list = np.asarray(te_loss_list)\n",
        "    gang_list = np.asarray(gang_list)\n",
        "    tr_corr_mean_list = np.asarray(tr_corr_mean_list)\n",
        "    tr_corr_std_list = np.asarray(tr_corr_std_list)\n",
        "\n",
        "    return Wn_list, l2_list, lr_list, dFdlr_list, gang_list, \\\n",
        "                tr_epoch, vl_epoch, te_epoch, tr_acc_list, te_acc_list, \\\n",
        "                tr_loss_list, vl_loss_list, te_loss_list, tr_corr_mean_list, tr_corr_std_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfHoNPP0HD7F"
      },
      "outputs": [],
      "source": [
        "def criterion(pred, target, model, lambda_l2_params):\n",
        "    loss = F.nll_loss(pred, target)\n",
        "    l2_penalty = sum((p**2).sum() * (lmbda / 2) for p, lmbda in zip(model.parameters(), lambda_l2_params))\n",
        "    return loss + l2_penalty\n",
        "\n",
        "def feval(data, target, model, optimizer, lambda_l2_params, mode='eval', is_cuda=0, opt_type='sgd', N=50000):\n",
        "\n",
        "    if mode == 'eval':\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = model(data)\n",
        "    else:\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "\n",
        "    loss = criterion(output, target, model, lambda_l2_params)\n",
        "    pred = output.argmax(dim=1, keepdim=True).flatten()\n",
        "    accuracy = pred.eq(target).float().mean()\n",
        "\n",
        "    grad_vec = []\n",
        "    noise = None\n",
        "    if 'train' in mode:\n",
        "        loss.backward()\n",
        "\n",
        "        for i,param in enumerate(model.parameters()):\n",
        "            if opt_type == 'sgld':\n",
        "                noise = torch.randn(size=param.shape)\n",
        "                if type(model.eta) == type(np.array([])):\n",
        "                    eps = np.sqrt(model.eta[i]*2/ N) * noise  if model.eta[i] > 0 else 0 * noise\n",
        "                else:\n",
        "                    eps = np.sqrt(model.eta*2/ N) * noise  if model.eta > 0 else 0 * noise\n",
        "                eps = to_torch_variable(eps, is_cuda=is_cuda)\n",
        "                param.grad.data = param.grad.data + eps.data\n",
        "            grad_vec.append(param.grad.data.cpu().numpy().flatten())\n",
        "\n",
        "        if 'SGD_Quotient_LR' in str(optimizer):\n",
        "            optimizer.mlp_step()\n",
        "        else:\n",
        "            optimizer.step()\n",
        "        grad_vec = np.hstack(grad_vec)\n",
        "        grad_vec = grad_vec / norm_np(grad_vec)\n",
        "\n",
        "    elif 'grad' in mode:\n",
        "        loss.backward()\n",
        "\n",
        "    return model, loss.item(), accuracy.item(), output, noise, grad_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnKpc2olg3bf"
      },
      "outputs": [],
      "source": [
        "def update_optimizer_hyperparams(model, optimizer):\n",
        "    optimizer.param_groups[0]['lr'] = np.copy(model.eta)\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alY567qoHKGj"
      },
      "outputs": [],
      "source": [
        "# evograd parameters\n",
        "n_model_candidates = 2\n",
        "sigma = 0.001\n",
        "temperature = 0.05\n",
        "\n",
        "def meta_update(data_vl, target_vl, data_tr, target_tr, model, optimizer, evo_grad_opt, lambda_l2_params, noise=None, is_cuda=1):\n",
        "    param_shapes = model.param_shapes\n",
        "    dFdlr = unflatten_array(model.dFdlr, model.param_cumsum, param_shapes)\n",
        "    Hv_lr  = compute_HessianVectorProd(model, dFdlr, data_tr, target_tr, is_cuda=is_cuda)\n",
        "\n",
        "    model, loss_valid, grad_valid = get_grad_valid(model, data_vl, target_vl, is_cuda)\n",
        "\n",
        "    grad = flatten_array(get_grads(model.parameters(), is_cuda)).data\n",
        "    param = flatten_array(model.parameters())#.data.cpu().numpy()\n",
        "    model.grad_norm = norm(grad)\n",
        "    model.param_norm = norm(param)\n",
        "    grad_vl = flatten_array(grad_valid)\n",
        "    model.grad_angle = torch.dot(grad / model.grad_norm, grad_vl / model.grad_norm_vl).item()\n",
        "\n",
        "    model.update_dFdlr(Hv_lr, param, grad, lambda_l2_params, is_cuda, noise=noise)\n",
        "    model.update_eta(mlr, val_grad=grad_valid)\n",
        "    param = flatten_array_w_0bias(model.parameters()).data\n",
        "\n",
        "    model_parameters = [i.detach() for i in model.parameters()]\n",
        "    theta_list = [[j + sigma*torch.sign(torch.randn_like(j)) for j in model_parameters] for i in range(n_model_candidates)]\n",
        "    pred_list = [model_patched(model, theta, data_vl) for theta in theta_list]\n",
        "    loss_list = [criterion(pred, target_vl, model, lambda_l2_params) for pred in pred_list]\n",
        "    weights = torch.softmax(-torch.stack(loss_list) / temperature, 0)\n",
        "    theta_updated = [sum(map(mul, theta, weights)) for theta in zip(*theta_list)]\n",
        "    preds_meta = model_patched(model, theta_updated, data_vl)\n",
        "    loss_l2 = loss(preds_meta, target_vl, model)\n",
        "    evo_grad_opt.zero_grad()\n",
        "    grads = torch.autograd.grad(loss_l2, lambda_l2_params.parameters(), retain_graph=True)\n",
        "    for p, g in zip(lambda_l2_params.parameters(), grads):\n",
        "        p.grad = g\n",
        "    evo_grad_opt.step()\n",
        "    with torch.no_grad():\n",
        "      for p in lambda_l2_params:\n",
        "          p.clamp_(min=0.0, max=0.0001)\n",
        "    \n",
        "    optimizer = update_optimizer_hyperparams(model, optimizer)\n",
        "\n",
        "    return model, loss_valid, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mylS45p-Jb8W"
      },
      "outputs": [],
      "source": [
        "def get_grad_valid(model, data, target, is_cuda):\n",
        "\n",
        "    val_model = deepcopy(model)\n",
        "    val_model.train()\n",
        "\n",
        "    output = val_model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    grads = get_grads(val_model.parameters(), is_cuda)\n",
        "    model.grad_norm_vl = norm(flatten_array(grads))\n",
        "\n",
        "    return model, loss, grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPOx0Yj5G4WH"
      },
      "source": [
        "Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX9CnyCEG14g",
        "outputId": "e16c44eb-e2a7-4b8b-f56c-cbfebfc121d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valid Epoch: 0, Loss 2.305242 Acc 0.095600\n",
            "Single epoch timing 0.418067\n",
            "Train Epoch: 0, Tr Loss 0.318316 Vl loss 0.328819 Acc 0.907600 Eta [0.10474716 0.10005984 0.10188739 0.10004982 0.10257516 0.10005449\n",
            " 0.10425777 0.10009306], L2 [Parameter containing:\n",
            "tensor(1.4864e-09, requires_grad=True), Parameter containing:\n",
            "tensor(3.0186e-12, requires_grad=True), Parameter containing:\n",
            "tensor(1.4676e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.0841e-11, requires_grad=True), Parameter containing:\n",
            "tensor(1.5180e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.0180e-11, requires_grad=True), Parameter containing:\n",
            "tensor(5.6399e-10, requires_grad=True), Parameter containing:\n",
            "tensor(4.2836e-12, requires_grad=True)], |dFdlr| 22.73 |G| 1.0606 |G_vl| 0.6352 Gang -0.117 |W| 13.85, Grad Corr 0.028311 0.196882\n",
            "Train Epoch: 1, Tr Loss 0.261353 Vl loss 0.253437 Acc 0.922000 Eta [0.10647327 0.10008544 0.10236875 0.100089   0.10295855 0.10009383\n",
            " 0.10463858 0.10012676], L2 [Parameter containing:\n",
            "tensor(2.2138e-09, requires_grad=True), Parameter containing:\n",
            "tensor(5.5667e-12, requires_grad=True), Parameter containing:\n",
            "tensor(2.1408e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.6780e-11, requires_grad=True), Parameter containing:\n",
            "tensor(2.2181e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.5307e-11, requires_grad=True), Parameter containing:\n",
            "tensor(8.6412e-10, requires_grad=True), Parameter containing:\n",
            "tensor(7.2186e-12, requires_grad=True)], |dFdlr| 31.03 |G| 0.9234 |G_vl| 0.6721 Gang 0.020 |W| 14.58, Grad Corr 0.000101 0.194838\n",
            "Train Epoch: 2, Tr Loss 0.196118 Vl loss 0.205377 Acc 0.943900 Eta [0.10843361 0.10008598 0.1030396  0.10011394 0.10326449 0.10012108\n",
            " 0.10487928 0.10014016], L2 [Parameter containing:\n",
            "tensor(4.4626e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.4098e-11, requires_grad=True), Parameter containing:\n",
            "tensor(4.1274e-09, requires_grad=True), Parameter containing:\n",
            "tensor(3.5379e-11, requires_grad=True), Parameter containing:\n",
            "tensor(4.2686e-09, requires_grad=True), Parameter containing:\n",
            "tensor(3.1344e-11, requires_grad=True), Parameter containing:\n",
            "tensor(1.8020e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.7207e-11, requires_grad=True)], |dFdlr| 36.84 |G| 1.0142 |G_vl| 0.6130 Gang -0.190 |W| 15.23, Grad Corr -0.000454 0.190372\n",
            "Train Epoch: 3, Tr Loss 0.167346 Vl loss 0.174718 Acc 0.949900 Eta [0.1101227  0.10008396 0.10361513 0.10012243 0.1035221  0.10013392\n",
            " 0.10506128 0.10014615], L2 [Parameter containing:\n",
            "tensor(5.8651e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.9751e-11, requires_grad=True), Parameter containing:\n",
            "tensor(5.3169e-09, requires_grad=True), Parameter containing:\n",
            "tensor(4.7097e-11, requires_grad=True), Parameter containing:\n",
            "tensor(5.4870e-09, requires_grad=True), Parameter containing:\n",
            "tensor(4.1271e-11, requires_grad=True), Parameter containing:\n",
            "tensor(2.3866e-09, requires_grad=True), Parameter containing:\n",
            "tensor(2.3479e-11, requires_grad=True)], |dFdlr| 41.04 |G| 0.6421 |G_vl| 0.6975 Gang -0.036 |W| 15.81, Grad Corr -0.000744 0.183640\n",
            "Train Epoch: 4, Tr Loss 0.139546 Vl loss 0.148347 Acc 0.959400 Eta [0.11149806 0.10008884 0.10412847 0.10012798 0.10373743 0.10014203\n",
            " 0.10521966 0.10014819], L2 [Parameter containing:\n",
            "tensor(6.3286e-09, requires_grad=True), Parameter containing:\n",
            "tensor(2.1638e-11, requires_grad=True), Parameter containing:\n",
            "tensor(5.6861e-09, requires_grad=True), Parameter containing:\n",
            "tensor(5.0922e-11, requires_grad=True), Parameter containing:\n",
            "tensor(5.8576e-09, requires_grad=True), Parameter containing:\n",
            "tensor(4.4451e-11, requires_grad=True), Parameter containing:\n",
            "tensor(2.5768e-09, requires_grad=True), Parameter containing:\n",
            "tensor(2.5470e-11, requires_grad=True)], |dFdlr| 44.95 |G| 0.8721 |G_vl| 0.5832 Gang -0.296 |W| 16.31, Grad Corr -0.001032 0.176923\n",
            "Train Epoch: 5, Tr Loss 0.124563 Vl loss 0.141386 Acc 0.964300 Eta [0.11263458 0.10010456 0.104453   0.10011512 0.10390645 0.10013474\n",
            " 0.10537198 0.10014182], L2 [Parameter containing:\n",
            "tensor(5.4356e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.7892e-11, requires_grad=True), Parameter containing:\n",
            "tensor(4.9979e-09, requires_grad=True), Parameter containing:\n",
            "tensor(4.3711e-11, requires_grad=True), Parameter containing:\n",
            "tensor(5.1735e-09, requires_grad=True), Parameter containing:\n",
            "tensor(3.8408e-11, requires_grad=True), Parameter containing:\n",
            "tensor(2.2129e-09, requires_grad=True), Parameter containing:\n",
            "tensor(2.1686e-11, requires_grad=True)], |dFdlr| 48.77 |G| 1.0077 |G_vl| 0.6233 Gang -0.029 |W| 16.76, Grad Corr -0.001163 0.176831\n",
            "Train Epoch: 6, Tr Loss 0.110779 Vl loss 0.128443 Acc 0.967900 Eta [0.11353411 0.10012865 0.10478365 0.10010445 0.10404991 0.10012834\n",
            " 0.10552145 0.10013606], L2 [Parameter containing:\n",
            "tensor(5.6724e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.8839e-11, requires_grad=True), Parameter containing:\n",
            "tensor(5.1785e-09, requires_grad=True), Parameter containing:\n",
            "tensor(4.5578e-11, requires_grad=True), Parameter containing:\n",
            "tensor(5.3546e-09, requires_grad=True), Parameter containing:\n",
            "tensor(4.0010e-11, requires_grad=True), Parameter containing:\n",
            "tensor(2.3123e-09, requires_grad=True), Parameter containing:\n",
            "tensor(2.2673e-11, requires_grad=True)], |dFdlr| 53.23 |G| 0.3993 |G_vl| 0.6107 Gang 0.155 |W| 17.17, Grad Corr -0.001183 0.179362\n",
            "Train Epoch: 7, Tr Loss 0.094994 Vl loss 0.121349 Acc 0.971100 Eta [0.11441352 0.1001714  0.10494005 0.10009378 0.10417019 0.10012408\n",
            " 0.10566356 0.10013037], L2 [Parameter containing:\n",
            "tensor(5.3123e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.7371e-11, requires_grad=True), Parameter containing:\n",
            "tensor(4.9306e-09, requires_grad=True), Parameter containing:\n",
            "tensor(4.2813e-11, requires_grad=True), Parameter containing:\n",
            "tensor(5.1173e-09, requires_grad=True), Parameter containing:\n",
            "tensor(3.7797e-11, requires_grad=True), Parameter containing:\n",
            "tensor(2.1761e-09, requires_grad=True), Parameter containing:\n",
            "tensor(2.1301e-11, requires_grad=True)], |dFdlr| 57.62 |G| 0.9318 |G_vl| 0.4380 Gang -0.055 |W| 17.56, Grad Corr -0.001258 0.174800\n",
            "Train Epoch: 8, Tr Loss 0.082546 Vl loss 0.111607 Acc 0.975000 Eta [0.11510815 0.10021828 0.10510989 0.10007959 0.10426273 0.10011769\n",
            " 0.10580636 0.10012339], L2 [Parameter containing:\n",
            "tensor(3.5820e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.0013e-11, requires_grad=True), Parameter containing:\n",
            "tensor(3.7128e-09, requires_grad=True), Parameter containing:\n",
            "tensor(2.9650e-11, requires_grad=True), Parameter containing:\n",
            "tensor(3.9440e-09, requires_grad=True), Parameter containing:\n",
            "tensor(2.7085e-11, requires_grad=True), Parameter containing:\n",
            "tensor(1.4985e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.4788e-11, requires_grad=True)], |dFdlr| 63.16 |G| 0.9369 |G_vl| 0.5880 Gang -0.359 |W| 17.92, Grad Corr -0.001277 0.170696\n",
            "Train Epoch: 9, Tr Loss 0.072552 Vl loss 0.107427 Acc 0.979900 Eta [0.11577422 0.10028304 0.10520163 0.10006467 0.10429788 0.10011216\n",
            " 0.10595621 0.10011824], L2 [Parameter containing:\n",
            "tensor(4.1391e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.2362e-11, requires_grad=True), Parameter containing:\n",
            "tensor(4.0944e-09, requires_grad=True), Parameter containing:\n",
            "tensor(3.3778e-11, requires_grad=True), Parameter containing:\n",
            "tensor(4.3069e-09, requires_grad=True), Parameter containing:\n",
            "tensor(3.0460e-11, requires_grad=True), Parameter containing:\n",
            "tensor(1.7130e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.6855e-11, requires_grad=True)], |dFdlr| 69.17 |G| 0.5554 |G_vl| 0.6077 Gang -0.010 |W| 18.26, Grad Corr -0.001263 0.168196\n",
            "Valid Epoch: 10, Loss 0.090864 Acc 0.971200\n",
            "Train Epoch: 10, Tr Loss 0.065964 Vl loss 0.104452 Acc 0.980000 Eta [0.11637662 0.10035735 0.10520349 0.10003957 0.10434081 0.10009865\n",
            " 0.10609109 0.10010848], L2 [Parameter containing:\n",
            "tensor(3.8737e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.1240e-11, requires_grad=True), Parameter containing:\n",
            "tensor(3.9119e-09, requires_grad=True), Parameter containing:\n",
            "tensor(3.1731e-11, requires_grad=True), Parameter containing:\n",
            "tensor(4.1334e-09, requires_grad=True), Parameter containing:\n",
            "tensor(2.8831e-11, requires_grad=True), Parameter containing:\n",
            "tensor(1.6103e-09, requires_grad=True), Parameter containing:\n",
            "tensor(1.5885e-11, requires_grad=True)], |dFdlr| 74.99 |G| 0.6788 |G_vl| 0.9389 Gang -0.051 |W| 18.57, Grad Corr -0.001399 0.171085\n",
            "Final test loss 0.090864\n"
          ]
        }
      ],
      "source": [
        "model_type = \"amlp\"\n",
        "mlr = 0.00001\n",
        "num_epoch = 10\n",
        "reset_freq = 0\n",
        "update_freq = 1\n",
        "checkpoint_freq = 10\n",
        "opt_type = 'sgd'\n",
        "lr = 0.1\n",
        "lambda_l2 = 0.000\n",
        "is_cuda = 1\n",
        "num_layers = 5\n",
        "hdims = [784] + [128]*3 + [10]\n",
        "fdir = '../save_dir'\n",
        "\n",
        "model = AMLP(num_layers, hdims, lr, is_cuda=is_cuda)\n",
        "optimizer = SGD_Multi_LR(model.parameters(), lr=lr)\n",
        "Wn_list, l2_list, lr_list, dFdlr_list, gang_list, tr_epoch, vl_epoch, te_epoch,\\\n",
        "                            tr_acc_list, te_acc_list, tr_loss_list, vl_loss_list, te_loss_list,\\\n",
        "                            tr_corr_mean_list, tr_corr_std_list \\\n",
        "                            = train(model_type, mlr, num_epoch, reset_freq, update_freq, checkpoint_freq, opt_type, dataset, model, optimizer, fdir, is_cuda=is_cuda)\n",
        "\n",
        "print('Final test loss %f' % te_loss_list[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VknBPPeNz9T",
        "outputId": "7538a429-3da3-444f-9dd2-ca7a30df70a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Type: amlp Opt Type: sgd Update Freq 1 Reset Freq 0\n",
            "../../save_dir/exp/mnist/mlr0.000010_lr0.100000_l20.000000/amlp_10epoch_100vlbz_sgd_1updatefreq_0resetfreq_fold0/\n",
            "Valid Epoch: 0, Loss 2.305165 Acc 0.097500\n",
            "Single epoch timing 0.451251\n",
            "Train Epoch: 0, Tr Loss 0.318154 Vl loss 0.348746 Acc 0.905900 Eta [0.10505738 0.10005581 0.1018957  0.10003892 0.10257875 0.10003885\n",
            " 0.10419294 0.10005693], L2 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 1.72420451e-08 1.58655837e-07 1.65552425e-08], |dFdlr| 22.78 |dFdl2| 1179.96 |G| 0.9697 |G_vl| 0.9494 Gang -0.160 |W| 13.86, Grad Corr 0.030127 0.193183\n",
            "Train Epoch: 1, Tr Loss 0.258982 Vl loss 0.262102 Acc 0.924300 Eta [0.10675105 0.10011777 0.10244388 0.10008138 0.10310605 0.1000797\n",
            " 0.10470598 0.10010963], L2 [0.00000000e+00 9.21795023e-08 0.00000000e+00 2.85593455e-07\n",
            " 0.00000000e+00 1.65916662e-07 1.38395976e-07 8.37083736e-08], |dFdlr| 32.04 |dFdl2| 2386.21 |G| 0.7515 |G_vl| 0.9013 Gang 0.163 |W| 14.60, Grad Corr 0.000245 0.196194\n",
            "Train Epoch: 2, Tr Loss 0.194793 Vl loss 0.206894 Acc 0.939800 Eta [0.10902197 0.10019453 0.10302859 0.10012184 0.103567   0.1001136\n",
            " 0.10504727 0.10014226], L2 [0.00000000e+00 0.00000000e+00 0.00000000e+00 2.06593967e-07\n",
            " 0.00000000e+00 1.99101137e-07 7.45105865e-07 9.29518134e-08], |dFdlr| 38.35 |dFdl2| 3610.23 |G| 0.8832 |G_vl| 0.9231 Gang -0.181 |W| 15.28, Grad Corr -0.000320 0.186228\n",
            "Train Epoch: 3, Tr Loss 0.154857 Vl loss 0.172213 Acc 0.953100 Eta [0.11101568 0.10025675 0.1035278  0.10014951 0.10392156 0.10013539\n",
            " 0.10526988 0.10016018], L2 [0.00000000e+00 2.97988507e-07 0.00000000e+00 2.01794180e-07\n",
            " 2.95068560e-07 5.03468367e-07 1.45715353e-06 3.88223025e-07], |dFdlr| 41.88 |dFdl2| 4861.36 |G| 1.2022 |G_vl| 0.9953 Gang -0.433 |W| 15.86, Grad Corr -0.000679 0.174480\n",
            "Train Epoch: 4, Tr Loss 0.130356 Vl loss 0.148789 Acc 0.960900 Eta [0.11255978 0.10032545 0.10381447 0.10018498 0.10417884 0.10015804\n",
            " 0.10543762 0.10017446], L2 [0.00000000e+00 6.44182322e-07 0.00000000e+00 4.66394695e-07\n",
            " 4.66281932e-07 4.68240398e-07 1.31325544e-06 1.82996168e-07], |dFdlr| 44.84 |dFdl2| 6137.30 |G| 0.5926 |G_vl| 0.9990 Gang 0.317 |W| 16.38, Grad Corr -0.000855 0.169824\n",
            "Train Epoch: 5, Tr Loss 0.108669 Vl loss 0.134469 Acc 0.965700 Eta [0.11372885 0.10039122 0.10395749 0.10022841 0.10437454 0.1001868\n",
            " 0.10554626 0.10019008], L2 [0.00000000e+00 3.50510998e-07 0.00000000e+00 4.91327341e-07\n",
            " 0.00000000e+00 5.42670078e-07 7.88047373e-07 2.81933080e-07], |dFdlr| 48.13 |dFdl2| 7430.69 |G| 0.8695 |G_vl| 0.8828 Gang -0.357 |W| 16.84, Grad Corr -0.001206 0.180208\n",
            "Train Epoch: 6, Tr Loss 0.105113 Vl loss 0.126755 Acc 0.968400 Eta [0.11472462 0.10046398 0.10392849 0.10027213 0.10449249 0.10021443\n",
            " 0.10563375 0.10020389], L2 [0.00000000e+00 1.91999046e-06 0.00000000e+00 8.01936274e-07\n",
            " 1.08136794e-06 9.03115573e-07 4.92213806e-06 4.63315339e-07], |dFdlr| 51.22 |dFdl2| 8741.68 |G| 0.9732 |G_vl| 0.9589 Gang 0.055 |W| 17.25, Grad Corr -0.001183 0.177392\n",
            "Train Epoch: 7, Tr Loss 0.099291 Vl loss 0.115316 Acc 0.970900 Eta [0.11538036 0.10054727 0.1039192  0.10032318 0.10456966 0.10024473\n",
            " 0.10569818 0.10021757], L2 [0.00000000e+00 2.81917426e-06 0.00000000e+00 2.48729149e-06\n",
            " 1.71141153e-06 2.00820987e-06 7.13300892e-06 1.01990311e-06], |dFdlr| 54.87 |dFdl2| 10061.50 |G| 0.9238 |G_vl| 1.1883 Gang -0.374 |W| 17.63, Grad Corr -0.001275 0.173853\n",
            "Train Epoch: 8, Tr Loss 0.079846 Vl loss 0.107972 Acc 0.975200 Eta [0.11589921 0.10063405 0.10388635 0.100382   0.1046314  0.10027987\n",
            " 0.1057502  0.10023313], L2 [0.00000000e+00 1.61077407e-06 0.00000000e+00 2.25646660e-06\n",
            " 7.29715964e-07 2.03683080e-06 5.05328636e-05 8.47125220e-07], |dFdlr| 58.28 |dFdl2| 11390.75 |G| 0.4116 |G_vl| 0.8776 Gang -0.094 |W| 17.98, Grad Corr -0.001238 0.173244\n",
            "Train Epoch: 9, Tr Loss 0.068925 Vl loss 0.110968 Acc 0.977800 Eta [0.11623828 0.10074146 0.10383373 0.1004432  0.10467006 0.10031377\n",
            " 0.1057988  0.1002467 ], L2 [0.00000000e+00 2.83513463e-06 0.00000000e+00 3.85725440e-06\n",
            " 3.36281370e-06 1.80974325e-06 1.32370389e-04 1.04142143e-06], |dFdlr| 62.19 |dFdl2| 12731.40 |G| 0.8945 |G_vl| 1.3209 Gang -0.258 |W| 18.30, Grad Corr -0.001298 0.167990\n",
            "Valid Epoch: 10, Loss 0.098614 Acc 0.970600\n",
            "Train Epoch: 10, Tr Loss 0.071425 Vl loss 0.103363 Acc 0.978900 Eta [0.11629419 0.10084679 0.10375996 0.10049891 0.10466875 0.10034453\n",
            " 0.10584824 0.10025925], L2 [7.70200042e-05 3.24957467e-06 4.58717443e-06 3.70542729e-06\n",
            " 4.41513436e-07 2.90109334e-06 2.36569002e-04 1.39873255e-06], |dFdlr| 66.26 |dFdl2| 14064.70 |G| 0.6420 |G_vl| 1.2490 Gang -0.046 |W| 18.58, Grad Corr -0.001224 0.166998\n",
            "Final test loss 0.098614\n",
            "<class 'numpy.float64'>\n"
          ]
        }
      ],
      "source": [
        "!python -u main.py --is_cuda 1 --mlr 0.00001 --lr 0.1 --lambda_l2 0.0000 --opt_type sgd --update_freq 1 --save 1  --model_type amlp --num_epoch 10 --batch_size_vl 100 --save_dir '../../save_dir'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
